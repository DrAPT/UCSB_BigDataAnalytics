{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Data Preprocessing\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235  \n",
    "### Last Updated: Oct 22, 2018\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning PySpark, Chapters 3, 4\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "\n",
    "### OBJECTIVES\n",
    "- Provide additional practice working with RDDs, DataFrames, Schemas, SQL, Temp Tables  \n",
    "- Introduction to methods for data preprocessing\n",
    "\n",
    "### PREREQUISITES\n",
    "- RDDs\n",
    "- Spark DataFrames\n",
    "- Schemas\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.host', '5b2d7f2b6613'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '33721'),\n",
       " ('spark.app.name', 'data preprocessing'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1540231376869'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_data = os.path.join('/home/jovyan/UCSB_BigDataAnalytics/data/fraud/ccFraud.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data; notice we can directly read zipped csv format\n",
    "fraud = sc.textFile(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"custID\",\"gender\",\"state\",\"cardholder\",\"balance\",\"numTrans\",\"numIntlTrans\",\"creditLine\",\"fraudRisk\"',\n",
       " '1,1,35,1,3000,4,14,2,0',\n",
       " '2,2,2,1,0,9,0,18,0',\n",
       " '3,2,2,1,0,27,9,16,0',\n",
       " '4,1,15,1,0,12,0,5,0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = fraud.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"custID\",\"gender\",\"state\",\"cardholder\",\"balance\",\"numTrans\",\"numIntlTrans\",\"creditLine\",\"fraudRisk\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rec = fraud.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = fraud \\\n",
    "        .filter(lambda row: row != header) \\\n",
    "        .map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 35, 1, 3000, 4, 14, 2, 0],\n",
       " [2, 2, 2, 1, 0, 9, 0, 18, 0],\n",
       " [3, 2, 2, 1, 0, 27, 9, 16, 0],\n",
       " [4, 1, 15, 1, 0, 12, 0, 5, 0],\n",
       " [5, 1, 46, 1, 0, 11, 16, 7, 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a schema to pass to a Spark dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    *[\n",
    "        typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "        for h in header.split(',')\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: h[1:-1] indexing strips quotes from field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(custID,IntegerType,true),StructField(gender,IntegerType,true),StructField(state,IntegerType,true),StructField(cardholder,IntegerType,true),StructField(balance,IntegerType,true),StructField(numTrans,IntegerType,true),StructField(numIntlTrans,IntegerType,true),StructField(creditLine,IntegerType,true),StructField(fraudRisk,IntegerType,true)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = spark.createDataFrame(fraud, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[custID: int, gender: int, state: int, cardholder: int, balance: int, numTrans: int, numIntlTrans: int, creditLine: int, fraudRisk: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache the table as we will use it many times\n",
    "fraud_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fraud_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|     1|     1|   35|         1|   3000|       4|          14|         2|        0|\n",
      "|     2|     2|    2|         1|      0|       9|           0|        18|        0|\n",
      "|     3|     2|    2|         1|      0|      27|           9|        16|        0|\n",
      "|     4|     1|   15|         1|      0|      12|           0|         5|        0|\n",
      "|     5|     1|   46|         1|      0|      11|          16|         7|        0|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create view so we can query using SQL\n",
    "fraud_df.createOrReplaceTempView(\"fraud_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10000000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of using SQL to count records\n",
    "spark.sql(\"select count(*) from fraud_v\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|     6|     2|   44|         2|   5546|      21|           0|        13|        0|\n",
      "|     8|     1|   10|         1|   6016|      20|           3|         6|        0|\n",
      "|    11|     1|   46|         1|   4601|      54|           0|         4|        0|\n",
      "|    14|     2|   38|         1|   9000|      41|           3|         8|        0|\n",
      "|    15|     1|   27|         1|   5227|      60|           0|        17|        0|\n",
      "|    17|     2|   18|         1|  13970|      20|           0|        13|        0|\n",
      "|    18|     1|   35|         1|   3113|      13|           6|         8|        0|\n",
      "|    19|     1|    5|         1|   9000|      20|           2|         8|        0|\n",
      "|    21|     1|   39|         1|   4000|      24|           0|         3|        0|\n",
      "|    26|     2|   29|         1|   5000|       4|           9|         4|        0|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from fraud_v where balance > 3000\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- custID: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      " |-- cardholder: integer (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- numTrans: integer (nullable = true)\n",
      " |-- numIntlTrans: integer (nullable = true)\n",
      " |-- creditLine: integer (nullable = true)\n",
      " |-- fraudRisk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     1|6178231|\n",
      "|     2|3821769|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=10000000\n"
     ]
    }
   ],
   "source": [
    "# row count\n",
    "print('rows={}'.format(fraud_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=10000000\n"
     ]
    }
   ],
   "source": [
    "# distinct row count\n",
    "print('rows={}'.format(fraud_df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there were duplicates, you could issue:\n",
    "fraud_df = fraud_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+------------------+---------------+----------------+--------------------+------------------+-----------------+\n",
      "|custID_missing|gender_missing|state_missing|cardholder_missing|balance_missing|numTrans_missing|numIntlTrans_missing|creditLine_missing|fraudRisk_missing|\n",
      "+--------------+--------------+-------------+------------------+---------------+----------------+--------------------+------------------+-----------------+\n",
      "|           0.0|           0.0|          0.0|               0.0|            0.0|             0.0|                 0.0|               0.0|              0.0|\n",
      "+--------------+--------------+-------------+------------------+---------------+----------------+--------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each field, compute missing percentage\n",
    "\n",
    "fraud_df.agg(*[\n",
    "    (1 - F.count(c) / F.count('*')).alias(c + '_missing')\n",
    "    for c in fraud_df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:  \n",
    "alias() allow for field renaming  \n",
    "none of the fields have any missing  \n",
    "count('*') counts all rows  \n",
    "the * following agg() treats the list as a separate set of parameters passed to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more interesting small example with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = spark.createDataFrame([ (1, 143.5, 5.6, 28, 'M', 100000),\n",
    " (2, 167.2, 5.4, 45, 'M', None),\n",
    " (3, None , 5.2, None, None, None),\n",
    " (4, 144.5, 5.9, 33, 'M', None),\n",
    " (5, 133.2, 5.7, 54, 'F', None),\n",
    " (6, 124.1, 5.2, None, 'F', None),\n",
    " (7, 129.2, 5.3, 42, 'M', 76000),\n",
    " ], ['id', 'weight', 'height', 'age', 'gender', 'income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.agg(*[\n",
    "    (1 - F.count(c) / F.count('*')).alias(c + '_missing')\n",
    "    for c in df_miss.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since income is sparse, we drop from the df\n",
    "\n",
    "df_miss_no_income = df_miss.select([\n",
    " c for c in df_miss.columns if c != 'income'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute non-categorical fields with column mean\n",
    "# note: computes to pandas df, then dictionary\n",
    "\n",
    "means = df_miss_no_income.agg(*[F.mean(c).alias(c) \\\n",
    "                                for c in df_miss_no_income.columns if c != 'gender']) \\\n",
    "                                .toPandas().to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add impute value for gender\n",
    "\n",
    "means['gender'] = 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4,\n",
       " 'gender': 'missing'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             143.5|   5.6| 28|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.28333333333333|   5.2| 40|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 40|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = spark.createDataFrame([\n",
    " (1, 143.5, 5.3, 28),\n",
    " (2, 154.2, 5.5, 45),\n",
    " (3, 342.3, 5.1, 99),\n",
    " (4, 144.5, 5.5, 33),\n",
    " (5, 133.2, 5.4, 54),\n",
    " (6, 124.1, 5.1, 21),\n",
    " (7, 129.2, 5.3, 42),\n",
    " ], ['id', 'weight', 'height', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flag values less than Q1 - 1.5 IQR OR greater than Q3 + 1.5 IQR as outliers  \n",
    "where IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_outliers.columns if c != 'id']   # exclude id from features\n",
    "bounds = {} # will store lower and upper bounds for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "\n",
    "    bounds[col] = [\n",
    "     quantiles[0] - 1.5 * IQR,\n",
    "     quantiles[1] + 1.5 * IQR\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': [91.69999999999999, 191.7],\n",
       " 'height': [4.499999999999999, 6.1000000000000005],\n",
       " 'age': [-11.0, 93.0]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append outlier indicators to data table\n",
    "\n",
    "outliers = df_outliers.select(*['id'] + [\n",
    " (\n",
    " (df_outliers[c] < bounds[c][0]) | (df_outliers[c] > bounds[c][1]))\n",
    "    .alias(c + '_outlier') for c in cols\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+-----------+\n",
      "| id|weight_outlier|height_outlier|age_outlier|\n",
      "+---+--------------+--------------+-----------+\n",
      "|  1|         false|         false|      false|\n",
      "|  2|         false|         false|      false|\n",
      "|  3|          true|         false|       true|\n",
      "|  4|         false|         false|      false|\n",
      "|  5|         false|         false|      false|\n",
      "|  6|         false|         false|      false|\n",
      "|  7|         false|         false|      false|\n",
      "+---+--------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to apply a treatment to handle the outliers, such as:  \n",
    "1. removing the record  \n",
    "2. clipping the outlier value to something \"reasonable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
