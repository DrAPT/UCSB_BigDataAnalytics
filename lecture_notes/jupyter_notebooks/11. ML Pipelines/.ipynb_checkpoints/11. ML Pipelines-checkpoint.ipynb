{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../ucsb_logo_seal.png\"> \n",
    "\n",
    "## ML Pipelines\n",
    "### PSTAT 135 / 235: Big Data Analytics\n",
    "### University of California, Santa Barbara\n",
    "### Last Updated: Sep 4, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "**Sources:**  \n",
    "Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "https://spark.apache.org/docs/latest/ml-pipeline.html  \n",
    "http://blog.insightdatalabs.com/spark-pipelines-elegant-yet-powerful/  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to ML Pipelines  \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- ML Pipeline\n",
    "- `DataFrame`\n",
    "- `Transformer`\n",
    "- `Estimator`\n",
    "- `Parameter`\n",
    "\n",
    "---\n",
    "\n",
    "**DataFrame**\n",
    "\n",
    "The ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. For example, columns can store text, feature vectors, true labels, and predictions.  Similar to the data frames in R and Python.\n",
    "\n",
    "Can be created from an RDD\n",
    "\n",
    "Columns are named\n",
    "\n",
    "**Transformer**  \n",
    "Transforms one DataFrame into another DataFrame\n",
    "\n",
    "**Estimator**  \n",
    "An algorithm that can be fit on a DataFrame (e.g., Logistic Regression)\n",
    "\n",
    "**Parameter**  \n",
    "Properties of an estimator (e.g., max number of iterations, regularization parameter)\n",
    "\n",
    "*Setter methods* are available for setting parameters:\n",
    "\n",
    "Set some params for logistic regression instance  \n",
    "\n",
    "`lr.setMaxIter(10)\n",
    "  .setRegParam(0.01)`\n",
    "\n",
    "**Pipeline**  \n",
    "A sequential chain of multiple `Transformers` and `Estimators` to specify an ML workflow  \n",
    "\n",
    "The pipeline in Spark is very similar to the pipeline in scikit-learn  \n",
    "Acts as a workflow to keep all steps together from start to finish, for example:\n",
    "- Data preprocessing\n",
    "- Feature extraction\n",
    "- Model tuning\n",
    "- Model fitting\n",
    "\n",
    "Keeping track of these steps manually can be painful and error-prone.  \n",
    "For example, the analyst might train on the test set by accident.  \n",
    "Pipelines make the process easily repeatable, and safer.  \n",
    "When new data comes along for scoring, it can be pushed through the pipeline.  \n",
    "\n",
    "**Pipeline Schematic**  \n",
    "`Cylinders` are DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_pipeline_graph.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA OUTLINE\n",
    "train_df  dataframe containing labels, restaurant reviews (string), ratings (integer) \n",
    "             will be used to train LogReg model\n",
    "test_df   dataframe with the same fields, set aside for model evaluation\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Configure pipeline stages\n",
    "# process review data\n",
    "tok = Tokenizer(inputCol=\"review\", outputCol=\"words\")  \n",
    "htf = HashingTF(inputCol=\"words\", outputCol=\"tf\", numFeatures=200)  \n",
    "\n",
    "# process review data\n",
    "w2v = Word2Vec(inputCol=\"review\", outputCol=\"w2v\")  \n",
    "\n",
    "# process rating data\n",
    "ohe = OneHotEncoder(inputCol=\"rating\", outputCol=\"rc\")  \n",
    "\n",
    "va = VectorAssembler(inputCols=[\"tf\", \"w2v\", \"rc\"], outputCol=\"features\")  \n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tok, htf, w2v, ohe, va, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.transform(test_df)\n",
    "\n",
    "# source: http://blog.insightdatalabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, the pipeline outlines the steps that will take place sequentially: \n",
    "\n",
    "1. The data is processed into features  \n",
    "2. The features are combined using `VectorAssembler`  \n",
    "3. The combined features are input to the Logistic Regression model  \n",
    "\n",
    "Calling `pipeline.fit(train_df)` will actually execute the workflow  \n",
    "\n",
    "Each step is either a `Transformer` or an `Estimator`  \n",
    "\n",
    "Each of the preprocessing steps is a `Transformer`  \n",
    "The logistic regression is an `Estimator`  \n",
    "\n",
    "**Another Pipeline Example:**  \n",
    "https://spark.apache.org/docs/1.6.0/ml-guide.html  \n",
    "\n",
    "**Custom Transformers**  \n",
    "There are many transformers available in `MLlib`  \n",
    "Users can also create custom transformers.  \n",
    "\n",
    "`Transformer` requirements:  \n",
    "\n",
    "1. Implement the `transform` method  \n",
    "2. Specify an `inputCol` and `outputCol`  \n",
    "3. Accept a DataFrame as input and return a DataFrame as output  \n",
    "\n",
    "\n",
    "**Saving and Loading Pipeline**  \n",
    "Pipelines can be saved for use later  \n",
    "This is helpful in several circumstances, including:  \n",
    "\n",
    "1. The user wishes to return to model development at a later time  \n",
    "2. Calling the pipeline to score records in production\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
