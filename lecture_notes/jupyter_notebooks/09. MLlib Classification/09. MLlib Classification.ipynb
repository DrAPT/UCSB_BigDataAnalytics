{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../ucsb_logo_seal.png\"> \n",
    "\n",
    "## MLlib Classification\n",
    "### PSTAT 135 / 235: Big Data Analytics\n",
    "### University of California, Santa Barbara\n",
    "### Last Updated: Sep 4, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "**Sources:**  \n",
    "Learning Spark, Chapter 11: Machine Learning with MLlib\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-naive-bayes.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-decision-tree.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-ensembles.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n",
    "\n",
    "https://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/mllib/util/MLUtils.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduce the major classification methods in MLlib\n",
    " \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Supervised learning\n",
    "- Binary and multiclass classification\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Tree methods\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient-Boosted Trees\n",
    "- Ensemble\n",
    "- Support vector machines\n",
    "\n",
    "---\n",
    "\n",
    "**Introduction to Classification**\n",
    "\n",
    "Classification is a common form of supervised learning.  \n",
    "In supervised learning, the training examples include labels.  \n",
    "After training the model, the purpose of the task is to predict labels for new examples.  \n",
    "\n",
    "The data type of the $Y$ variable makes it a *classification problem*, namely $Y$ is a discrete variable.  \n",
    "Binary classification is most common. Examples include fraud (or not), default, survival, claim filing, spam.\n",
    "\n",
    "A continuous $Y$ variable results in a regression problem (next topic).\n",
    "\n",
    "Classification and regression both use the `LabeledPoint` class.  \n",
    "To remind ourselves, a `LabeledPoint` consists of a label and a features vector.  \n",
    "\n",
    "Follow this convention for labels:  \n",
    "- For binary classification, use labels $0$ and $1$  \n",
    "- For multiclass classification, use labels $0$, $1$, …, $C-1$ where $C$ is the number of classes  \n",
    "\n",
    "\n",
    "\n",
    "Spark supports several popular models for classification including:  \n",
    "- Logistic regression  \n",
    "- Naive Bayes  \n",
    "- Tree methods (e.g., decision tree, random forest)  \n",
    "- Support Vector Machines  \n",
    "\n",
    "\n",
    "\n",
    "**Logistic regression**    \n",
    "This is currently the most popular method for binary classification.  \n",
    "It is a generalized linear model which uses a linear plane to separate positive and negative examples.  \n",
    "Although the model is relatively simple, the results can be very competitive.  \n",
    "\n",
    "Below is an example of some data and a logistic curve fit to the data. Probability of Passing $Y$ is a function of Hours Studying $X$.  Notice the $Y$ variable consists of the values 0, 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib includes the stochastic gradient descent (`SGD`) and `L-BFGS` algorithms for fitting the model.  \n",
    "`L-BFGS` is an approximation to Newton’s method that converges faster; it is the recommended method. \n",
    "\n",
    "From the documentation:\n",
    "\n",
    "“ `L-BFGS` version is strongly recommended since it converges faster and more accurately compared to `SGD` by approximating the inverse Hessian matrix using quasi-Newton method.”\n",
    "\n",
    "Multiclass Problems  \n",
    "The algorithm will output a multinomial logistic regression model, which contains $K−1$ binary logistic regression models regressed against the first class. Given a new data point, $K−1$ models will be run, and the class with largest probability will be chosen as the predicted class.\n",
    "\n",
    "**Logistic Regression Implementation**\n",
    "\n",
    "The names of the model fitting functions include the algorithm applied, when there is a choice.  \n",
    "For logistic regression, the following functions are supported:\n",
    "\n",
    "- `LogisticRegressionWithLBFGS`\n",
    "- `LogisticRegressionWithSGD`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**  \n",
    "\n",
    "Naive Bayes (NB) is a relatively simple model, yet the performance can be quite good.  This has led to its popularity.  \n",
    "\n",
    "NB does multiclass classification. It is commonly used in text classification where the input features are count variables.\n",
    "\n",
    "At a high level, the count of a word on a page can adjust the probability that the page belongs to a given class.  For example, the presence of the word “tacos” will increase the probability that the page belongs to a **restaurant** relative to a **florist**.\n",
    "\n",
    "The algorithm computes the conditional probability distribution of each feature given a label, and then it applies Bayes’ theorem to compute the conditional probability distribution of a label given an observation.\n",
    "\n",
    "Naive?  \n",
    "The term “naive” comes from the simplifying assumption of independence between every pair of features. This assumption greatly simplifies the model and is often reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Implementation`**  \n",
    "\n",
    "Two methods are supported:  \n",
    "\n",
    "- multinomial naive Bayes (function name: `NaiveBayes`)\n",
    "- Bernoulli naive Bayes\n",
    "\n",
    "**Parameters**  \n",
    "The model type is selected with an optional parameter “multinomial” or “bernoulli” with “multinomial” as the default.  \n",
    "\n",
    "Additive smoothing can be used by setting the parameter $λ$ (default = 1.0)  \n",
    "\n",
    "For document classification, the input feature vectors are usually sparse, and sparse vectors should be supplied as input to take advantage of sparsity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Example: load data/train model/predict**\n",
    "\n",
    "*Preamble*  \n",
    "\n",
    "The data used in this example is in libsvm format.  \n",
    "This has sparse format like this:  \n",
    "\n",
    "`0 1:51 2:159 3:253`\n",
    "\n",
    "`label index1:value1 index2:value2…`\n",
    "\n",
    "The data is read using:   `MLUtils.loadLibSVMFile`\n",
    "\n",
    "`MLUtils`  \n",
    "Helper methods to load, save and pre-process data used in `MLlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file. Note this data is in sparse format.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy = 1.0 * labelsAndPreds.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-naive-bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree Methods**  \n",
    "\n",
    "Tree methods can be used for both classification and regression  \n",
    "\n",
    "Simplest method is a Decision Tree, which is intuitively appealing due to series of binary decisions (Male/Female, Age greater than 30 or not)  \n",
    "\n",
    "Can handle missing values (in many implementations), categorical data, continuous data.  \n",
    "Minimal preprocessing needed.  \n",
    "\n",
    "Feature selection is part of algorithm (best feature is used, then next best, …)  \n",
    "\n",
    "Does not require scaling  \n",
    "Handles non-linear interactions  \n",
    "Handles multiclass classification  \n",
    "\n",
    "**Decision Tree Architecture**  \n",
    "\n",
    "Tree uses binary splits on one feature at a time  \n",
    "Top of tree is the root  \n",
    "Paths or branches emanate from nodes  \n",
    "Bottom layer of nodes called the leaf nodes, which contain predictions  \n",
    "\n",
    "**Decision Tree Implementation**  \n",
    "\n",
    "`mllib.tree.DecisionTree` class  \n",
    "`trainClassifier()`\n",
    "\n",
    "Implementation partitions data by rows, allowing distributed training with millions of instances\n",
    "\n",
    "Parameters  \n",
    "Node impurity measures the homogeneity of the labels in the leaf nodes.  \n",
    "Two options are available for classification: Gini impurity and entropy.\n",
    "\n",
    "**Decision Tree Example: load data/train model/predict**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-Based Ensemble Methods**\n",
    "\n",
    "*Ensembles* combine multiple models together to produce a new model.  \n",
    "They may consist of models of the same type (e.g., all decision trees) or mixed type (e.g., decision tree + neural net + svm)  \n",
    "\n",
    "One of the fundamental results in machine learning is that multiple weak classifiers can be combined to produce a strong classifier.  \n",
    "\n",
    "Ensembles are useful in reducing overfitting, since predictions are based on several different trees  \n",
    "\n",
    "The two most popular tree-based ensemble methods are *Random Forests* and *Boosted Trees* (e.g. *Gradient-Boosted Trees*)  \n",
    "\n",
    "They are popular because they are often very competitive  \n",
    "\n",
    "The nice properties of decision trees carry over to ensembles of trees  \n",
    "\n",
    "This combining step can proceed using different methods, including:  \n",
    "\n",
    "- voting (for classification)\n",
    "- averaging (for regression) \n",
    "- running model predictions through another model (classification and regression)\n",
    "\n",
    "There are downsides to ensembles:  \n",
    "\n",
    "- Multiple models need to be trained, loaded, and maintained  \n",
    "- Model explanation is harder: no p-values like regression, several trees are feeding overall decision.  \n",
    "There are methods to provide feature importance information, such as partial dependence plots.\n",
    "\n",
    "**Random Forest**  \n",
    "Ensembles of decision trees  \n",
    "\n",
    "RFs inject two sources of randomness into modeling:  \n",
    "\n",
    "1. At each step, randomly select $p$ features out of $n$ total features for possible inclusion (random subspace method)\n",
    "2. Sample the original training set with replacement, up to the size of the original training set (bootstrapping of the training set)\n",
    "\n",
    "The number of features to randomly select $p$ is a parameter  \n",
    "The number of bootstrapped trees to grow $N$ is a parameter  \n",
    "\n",
    "Since the trees are grown independently, the training and prediction tasks are embarrassingly parallel and can be assigned to multiple workers.\n",
    "\n",
    "Classification prediction done by majority vote across trees\n",
    "\n",
    "**Random Forest Implementation**\n",
    "\n",
    "`from pyspark.mllib.tree import RandomForest`  \n",
    "\n",
    "Two most important parameters (which should be tuned using $k$-fold cross validation):  \n",
    "\n",
    "- `numTrees`: Number of trees in forest\n",
    "More trees will increase accuracy but also training time  \n",
    "\n",
    "- `maxDepth`: Maximum depth of each tree in forest\n",
    "Increasing depth can increase power of model, but will take longer to train and can overfit  \n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `subsamplingRate`: fraction of size of original training set (default=1.0 recommended)\n",
    "\n",
    "- `featureSubsetStrategy`: specified as fraction or function of total number of features\n",
    "\n",
    "**Random Forest Example: load data/train model/predict**  \n",
    "NOTE: Very similar to Decision Tree code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient-Boosted Trees**  \n",
    "\n",
    "GBTs work by building a sequence of trees and combining their predictions at each iteration.  The trees constructed are generally *stumps* which use a single decision split.  A stump is an example of a weak learner.\n",
    "\n",
    "This is different from random forests, where each tree independently gives predictions on each training instance.\n",
    "\n",
    "\n",
    "\n",
    "A loss is specified and an optimization problem is solved whereby the objective is to minimize the loss of the model by adding weak learners using a gradient-descent-like procedure.\n",
    "\n",
    "The procedure follows a stage-wise additive model, meaning that one new weak learner is\n",
    "added at a time and existing weak learners are left unchanged.\n",
    "For the original work, see:\n",
    "\n",
    "*Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of Statistics (2001): 1189–1232.*\n",
    "\n",
    "\n",
    "**Gradient-Boosted Trees Implementation**  \n",
    "\n",
    "Since the trees are built in a sequential fashion, the algorithm can not be run in parallel.  \n",
    "However, shallow trees (e.g., stumps) can be used effectively; this saves time versus random forests, which use deeper trees.\n",
    "\n",
    "The loss function in classification problems is the log loss, equal to twice the binomial negative log likelihood.\n",
    "\n",
    "Important parameters:\n",
    "- `numIterations`:  equal to the number of trees in the ensemble.  More trees means longer runtime but also better performance up to a point.\n",
    "- `learningRate`:  how quickly the model adapts on each iteration. A smaller value may help the algo have better performance, but at the cost of additional runtime. The documentation recommends NOT tuning this param.\n",
    "\n",
    "The method `runWithValidation` can help mitigate overfitting.  It takes a training RDD and a validation RDD.\n",
    "\n",
    "The training is stopped when the improvement in the validation error is not more than a certain tolerance (supplied by the `validationTol` argument in `BoostingStrategy`).\n",
    "\n",
    "**GBT Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=1000)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Machine**  \n",
    "\n",
    "An SVM constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space.  \n",
    "\n",
    "Can be used for classification, regression, or other tasks.   \n",
    "Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data points (*support vectors*).\n",
    "\n",
    "**Linear Separation using SVM in the Classification Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"svm_img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Implementation**  \n",
    "\n",
    "Internally, it optimizes the `Hinge Loss` using OWLQN optimizer.  \n",
    "\n",
    "Important Parameters\n",
    "- `regParam`  regularization parameter\n",
    "\n",
    "**SVM Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linearsSVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
