{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../ucsb_logo_seal.png\"> \n",
    "\n",
    "## Running on a Cluster\n",
    "### PSTAT 135 / 235: Big Data Analytics\n",
    "### University of California, Santa Barbara\n",
    "### Last Updated: Sep 4, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "1. Learning Spark, Chapter 7\n",
    "\n",
    "### OBJECTIVES\n",
    "- Learn how to run distributed Spark\n",
    "- Learn about some of the common deployment environments\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Cluster manager (Hadoop YARN, Apache Mesos, Standalone)\n",
    "- Driver and worker/executor\n",
    "- Spark application\n",
    "- Directed acyclic graph (DAG)\n",
    "- Build tool\n",
    "- Assembly JAR\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Architecture\n",
    "\n",
    "One benefit of Spark is the ability to scale computation by adding more machines and running in cluster mode\n",
    "\n",
    "*Driver* is in charge of coordinating the workers\n",
    "\n",
    "The *workers* / *executors* receive code and data and do the processing, sending results back to driver.\n",
    "\n",
    "Driver + Workers = Spark application\n",
    "\n",
    "### Driver\n",
    "\n",
    "`main()` method of program runs on driver\n",
    "\n",
    "Converts program into tasks\n",
    "\n",
    "Converts into logical *directed acyclic graph* (DAG) of operations\n",
    "\n",
    "Coordinates scheduling of tasks on executors (like a manager)\n",
    "\n",
    "### Executors\n",
    "\n",
    "Run the individual tasks\n",
    "\n",
    "Launch at start of application and run for lifetime of app\n",
    "\n",
    "Provide in-memory (RAM) storage for RDDs\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "External service where the Spark application runs.  \n",
    "\n",
    "Spark is packaged with the Standalone cluster manager.\n",
    "\n",
    "Manages the resources between Spark applications.  \n",
    "Can manage queues if there is more demand than resources for executors.\n",
    " \n",
    "### Launching a Program\n",
    "`spark-submit` is called to launch a Spark app\n",
    "\n",
    "**Run in local mode using single core**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit --master local python_scripts\\textAnalysis1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using 4 cores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit --master local[4] python_scripts\\textAnalysis1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using all cores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit --master local[*] python_scripts\\textAnalysis1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Run on Spark Standalone cluster at default port**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit --master spark://host:7077 python_scripts\\textAnalysis1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Run on Spark Standalone cluster at default port, specifying memory to allocate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit --master spark://host:7077 –-executor_memory 10g \tpython_scripts\\textAnalysis1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generic Form to run Spark App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ bin\\spark-submit [options] <app jar | python file> [app options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can include various flags in the short or long format `-shortflag` and \n",
    "`--longflag` respectively  \n",
    "\n",
    "See page 122 for a list of flags  \n",
    "\n",
    "The flags control scheduling information and dependencies such as libraries and files\n",
    "\n",
    "For a list of all flags issue:  \n",
    "bin\\spark-submit --help\n",
    "\n",
    "\n",
    "### Spark Web UI\n",
    "\n",
    "Local mode:  \n",
    "http://localhost:4040/jobs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"spark_app_mgr.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging Code and Dependencies  \n",
    "\n",
    "**Python**  \n",
    "PySpark uses Python on worker machines, so can use `pip`  \n",
    "Can also submit libaries using `--py-files` argument to `spark-submit`  \n",
    "\n",
    "**Java and Scala**   \n",
    "can submit individual JAR files using `--jars`  \n",
    "For a large set of dependencies, better to use a build tool (`sbt` or `maven`) to package all dependencies into one JAR called the \n",
    "*assembly *JAR  \n",
    "\n",
    "Maven produces a pom.xml file containing a build definition\n",
    "\n",
    "A *Project Object Model* or *POM* is the fundamental unit of work in Maven. It is an XML file that contains information about the project and configuration details used by Maven to build the project. It contains default values for most projects.\n",
    "\n",
    "https://maven.apache.org/guides/introduction/introduction-to-the-pom.html\n",
    "\n",
    "Packaging a spark application built w Maven is straightforward:    \n",
    "\n",
    "**Run on Spark Standalone cluster at default port**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ mvn package          # create the assembly JAR\n",
    "\n",
    "# The assembly JAR will be placed in the target directory\n",
    "$ bin\\spark-submit --master local … target\\name_of_assembly.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop YARN\n",
    "\n",
    "**Y**et **A**nother **R**esource **N**egotiator \n",
    "\n",
    "YARN is a cluster manager introduced in Hadoop 2.0  \n",
    "Allocates system resources to various applications running in Hadoop cluster  \n",
    "Schedules tasks to be executed on different cluster nodes  \n",
    "Installed on same nodes as *HDFS*, making it quicker to access data  \n",
    "\n",
    "To use YARN in Spark, set an environment variable that points to Hadoop config directory, then submit jobs to a special master URL with `spark-submit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export HADOOP_CONF_DIR=\"...\"\n",
    "spark-submit --master yarn appname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default YARN uses 2 executors, so will likely need to change with flag:  \n",
    "`--num-executors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"yarn.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Resource Manager* accepts jobs from users, schedules them and allocates resources  \n",
    "\n",
    "*Node Manager* monitors the node and provides reporting\n",
    "\n",
    "*Application Master* created for each application to negotiate for resources and work with NodeManager to execute and monitor tasks\n",
    "\n",
    "*Containers* are controlled by the NodeManager and assigned system resources\n",
    "\n",
    "\n",
    "\n",
    "### Amazon EC2 (elastic cloud compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has built-in script to launch clusters on EC2: `spark-ec2`\n",
    "\n",
    "Will need Amazon Web Services (AWS) account  \n",
    "Export the *access key ID* and *secret access key*    \n",
    "By default, launching the cluster produces one master and one slave  \n",
    "Storage: Spark EC2 clusters include two installations of HDFS  \n",
    "See p 136 for details"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
