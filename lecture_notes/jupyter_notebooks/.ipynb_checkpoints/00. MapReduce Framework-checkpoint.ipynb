{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\", align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Framework\n",
    "\n",
    "### University of Virginia\n",
    "### [Course Number]: Big Data Analytics\n",
    "### Last Updated: Sep 4, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCE: \n",
    "\n",
    "Hadoop: The Definitive Guide, Tom White\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to the MapReduce framework\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- `MapReduce`\n",
    "- Jobs\n",
    "- Tasks\n",
    "- Map operation\n",
    "- Reduce operation\n",
    "- Shuffle operation\n",
    "\n",
    "---\n",
    "\n",
    "**Introducing MapReduce**\n",
    "\n",
    "`MapReduce` is a programming framework for processing large data sets with a parallel, distributed algorithm on a cluster.  \n",
    "\n",
    "First the data set is “mapped” into a collection of `<key, value>` pairs, and then “reduced” over all pairs with the same key.  We need to discuss what this means.  \n",
    "\n",
    "The operations are surprisingly broad, enabling them to handle a wide range of use cases.\n",
    "\n",
    "The popular minimal example is to do a word count on some text.\n",
    "\n",
    "Initially published in 2004 by employees at Google\n",
    "\n",
    "A `MapReduce` job is a unit of work the client wants to be performed  \n",
    "\n",
    "A job consists of:  \n",
    "\n",
    "- Input data\n",
    "- `MapReduce` program\n",
    "- configs\n",
    "\n",
    "The job will be divided into *tasks*  \n",
    "Two types of tasks: *map tasks* and *reduce tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Aside on `Hadoop`**  \n",
    "\n",
    "`MapReduce` forms the computation paradigm for Hadoop  \n",
    "\n",
    "`Hadoop` was created by Doug Cutting and Mike Cafarella.  Version 1 was called Nutch.  \n",
    "\n",
    "Yahoo! was instrumental in providing a dedicated team and resources to turn Hadoop into a system that ran at web scale.\n",
    "\n",
    "Hadoop is developed and maintained by the Apache Software Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on Hadoop and Spark**  \n",
    "\n",
    "Spark does not follow the `MapReduce` framework in the same way that `Hadoop` does, where `Mapper` and `Reducer` functions are detailed explicitly.  This is actually a big advantage of Spark.\n",
    "\n",
    "Spark and Hadoop can be better together.  For example, some teams will use Hadoop data storage (HDFS) with Spark.  That is, a Spark job can read data from HDFS, and write results to HDFS.\n",
    "\n",
    "Reported runtimes:\n",
    "Spark can be as much as 10 times faster than `MapReduce` for batch processing, and up to 100 times faster for in-memory analytics.\n",
    "\n",
    "A large fraction of this course will focus on Spark for various tasks.\n",
    "\n",
    "Next we illustrate the `MapReduce` framework with an example.\n",
    "\n",
    "**MapReduce Word Count Process Diagram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"map_reduce_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a minimal case Spark Session:\n",
    "- using the local machine as master\n",
    "- naming the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"pspark_test\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5b2d7f2b6613:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pspark_test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe3854a5128>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print info about the session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RDDs and Datasets\n",
    "\n",
    "Before Spark 2.0, the main programming interface of Spark was the *Resilient Distributed Dataset (RDD)*.  \n",
    "\n",
    "After Spark 2.0, RDDs are replaced by *Dataset*, which is strongly-typed like an RDD, but with richer optimizations under the hood. \n",
    "\n",
    "The RDD interface is still supported  \n",
    "\n",
    "Using Dataset is recommended, and it has better performance than RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing\n",
    "\n",
    "### Example 1: Read lines from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/jovyan/work/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filename = 'README.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(os.path.join(data_path, data_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Apache Spark')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines.collect()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Text Search  - print all lines containing “Spark”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['value']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_lines.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_lines = lines.filter(lines.value.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Apache Spark'),\n",
       " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
       " Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n",
       " Row(value='and Spark Streaming for stream processing.'),\n",
       " Row(value='You can find the latest Spark documentation, including a programming')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return list of first 5 records\n",
    "spark_lines.take(5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the file into an RDD\n",
    "lines = sc.textFile(os.path.join(data_path, data_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Apache', 'Spark', 'Spark', 'is']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 'the'),\n",
       " (17, 'to'),\n",
       " (16, 'Spark'),\n",
       " (12, 'for'),\n",
       " (9, 'and'),\n",
       " (9, '##'),\n",
       " (8, 'a'),\n",
       " (7, 'can'),\n",
       " (7, 'on'),\n",
       " (7, 'run')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
