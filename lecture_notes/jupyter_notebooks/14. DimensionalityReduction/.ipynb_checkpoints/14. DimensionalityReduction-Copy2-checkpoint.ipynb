{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ucsb_logo_seal.png\", align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235  \n",
    "### Last Updated: Aug 06, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "**Sources:**  \n",
    "Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html  \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/pca_rowmatrix_example.py  \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/svd_example.py  \n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Discuss purpose of dimensionality reduction  \n",
    "-  Introduce Principal Component Analysis $(PCA)$  \n",
    "-  Introduce Singular Value Decomposition $(SVD)$  \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Matrix rank  \n",
    "- Low-rank approximation of a matrix  \n",
    "- Dimensionality reduction  \n",
    "- $PCA$  \n",
    "- $SVD$  \n",
    "\n",
    "---\n",
    "\n",
    "**Rank of a Matrix**\n",
    "\n",
    "The **rank** of a matrix $A$ is the dimension of the vector space generated (or spanned) by its columns.  This corresponds to the maximal number of linearly independent columns of $A$.\n",
    "\n",
    "The **column rank** of $A$ is the dimension of the column space of $A$.  \n",
    "\n",
    "The **row rank** of $A$ is the dimension of the row space of $A$.  \n",
    "\n",
    "A fundamental result in linear algebra is that the column rank and the row rank are always equal.\n",
    "\n",
    "\n",
    "**Rank = 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{A} =  \\begin{bmatrix}\n",
    "1 & 1 & 0 & 2  \\\\\n",
    "-1 & -1 & 0 & -2  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rank = 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{A} =  \\begin{bmatrix}\n",
    "1 & 0 & 1  \\\\\n",
    "-2 & -3 & 1  \\\\\n",
    "3 & 3 & 0  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction** \n",
    "\n",
    "For a matrix of data with (independent) features along the columns, the dimension is equal to the number of features under consideration.  \n",
    "\n",
    "There are several reasons for reducing the number of dimensions including:  \n",
    "\n",
    "\n",
    "- Visualization  \n",
    "\n",
    "\n",
    "- $p >> n$ problem, or the curse of dimensionality (more features than observations)  \n",
    "\tInsufficient degrees of freedom to estimate a model\n",
    "\n",
    "\n",
    "- Saving on storage requirements  \n",
    "For example, a full matrix inversion step in regression (below) can be prohibitively large to store.\n",
    "\n",
    "\n",
    "$$ \\hat{β} = (X^TX)^{-1}X^TY $$\n",
    "\n",
    "\n",
    "Instead, the Gram matrix $X^{T}X$ can be replaced by a lower rank matrix decomposition from $SVD$, as an example.\n",
    "\n",
    "**Denoising the data** \n",
    "\n",
    "Even randomly generated data will produce a correlation matrix with extreme values by chance.  \n",
    "\n",
    "Compressing information into a smaller set of features will reduce noise  \n",
    "\n",
    "This is particularly useful when the covariance matrix is important \n",
    "(e.g., Mean Variance Portfolio Optimization in *Quantitative Finance*)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**  \n",
    "\n",
    "$PCA$ is the primary technique used for dimensionality reduction  \n",
    "\n",
    "At a high level, $PCA$ constructs a set of new vectors which are a linear combination of the original vectors.  These new vectors have two special properties:  \n",
    "\n",
    "1. The vectors form an orthogonal basis, which means they are uncorrelated  \n",
    "2. The first principal component has the largest variance (it accounts for the most variability in the data), the second components has the next largest variance (while also being orthogonal to the first component), and so on.\n",
    "\n",
    "\n",
    "**PCA Illustrated in Two Dimensions**  \n",
    "The principal components point in the directions of greatest variance.  They are also orthogonal.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pca_img.gif\", align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components will be equal to the starting number of vectors. However, only a subset of the components will be used (the top $k$).  \n",
    "\n",
    "**The top principal components can be used to create orthogonal, condensed features in a model.**  \n",
    "\n",
    "**Pitfalls of PCA**  \n",
    "$PCA$ includes a step where it computes the covariance matrix $X^{T}X$   \n",
    "This can lead to numerical rounding errors when calculating the eigenvalues/vectors.  \n",
    "\n",
    "For more details, please refer here for example:  \n",
    "http://en.wikipedia.org/wiki/Principal_component_analysis  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "# creates an RDD, as shown below\n",
    "type(rows)\n",
    "<class 'pyspark.rdd.RDD'>\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "# Compute the top 4 principal components.\n",
    "# Principal components are stored in a local dense matrix.\n",
    "pc = mat.computePrincipalComponents(4)\n",
    "\n",
    "# Project the rows to the linear space spanned by the top 4 principal components.\n",
    "projected = mat.multiply(pc)\n",
    "# Collect and print results\n",
    "collected = projected.rows.collect()\n",
    "print(\"Projected Row Matrix of principal component:\")\n",
    "for v in collected:\n",
    "  print(v)\n",
    "sc.stop()\n",
    "\n",
    "# Aside: can convert sparse vector to dense vector as follows:\n",
    "vs = Vectors.sparse(5, {1: 1.0, 3: 7.0})  # the sparse vector\n",
    "vd = Vectors.dense(vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition $(SVD)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SVD$ is one of the major accomplishments of linear algebra, and it is a popular technique for factorizing an *m x n* (rectangular) matrix $A$ into three matrices with special properties.\n",
    "\n",
    "\\begin{equation*}\n",
    "A   = U \\Sigma V^T\n",
    "\\end{equation*}\n",
    "\n",
    "$U$ is an orthonormal *m x m* matrix; its columns are called *left singular vectors*  \n",
    "$\\Sigma$ is a rectangular *m x n* diagonal matrix. It has nonnegative values in descending order  \n",
    "$V$ is an orthonormal *n x n* matrix; its columns are called *right singular vectors*  \n",
    "\n",
    "The diagonal entries of $\\Sigma$ are known as the *singular values* of $A$.  \n",
    "They are the square roots of the eigenvalues from the matrix $AA^T$\n",
    "\n",
    "The purpose of $SVD$ is to select only the top *k* singular values and their associated singular vectors.  \n",
    "This means that we select only subsets of the matrices, which we denote as $\\hat{U}$ , $\\hat{\\Sigma}$, and $\\hat{V}^T$.  \n",
    "An approximation to  $A$  can then be constructed as \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{A}   = \\hat{U} \\hat{\\Sigma} \\hat{V}^T\n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{U}$ has dimensions *m x k*  \n",
    "$\\hat{\\Sigma}$  has dimensions *k x k*  \n",
    "$\\hat{V}^T$ has dimensions *k x n*  \n",
    "\n",
    "The purpose of this factorization is to save on storage requirements, denoise, and recover the low-rank structure of the matrix.\n",
    "\n",
    "For more details, please refer here for example:  \n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition  \n",
    "\n",
    "- If $n$ is small $(n<100)$ or $k$ is large compared with $n (k > n/2)$, we compute the Gramian matrix first and then compute its top eigenvalues and eigenvectors locally on the driver. This requires a single pass with $O(n^2)$ storage on each executor, and $O(n^2k)$ time on the driver. \n",
    "\n",
    "\n",
    "- Otherwise, we compute $(A^TA)v$ in a distributed way and send it to $ARPACK$ to compute $(A^TA)$’s top eigenvalues and eigenvectors on the driver node. This requires $O(k)$ passes, $O(n)$ storage on each executor, and $O(nk)$ storage on the driver.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARPACK**  \n",
    "$ARPACK$ is a collection of `Fortran77` subroutines designed to solve large scale eigenvalue problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will run the Pi example locally.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$SVD$ Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top k singular values and corresponding singular vectors.\n",
    "topk = 4\n",
    "svd = mat.computeSVD(topk, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "print(\"Singular values are: %s\" % s)\n",
    "print(\"V factor is:\\n%s\" % V)\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
