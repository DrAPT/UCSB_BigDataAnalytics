{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../ucsb_logo_seal.png\">  \n",
    "\n",
    "## MapReduce Framework\n",
    "\n",
    "### PSTAT 135 / 235: Big Data Analytics\n",
    "### University of California, Santa Barbara\n",
    "### Last Updated: Sep 4, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCE: \n",
    "\n",
    "Hadoop: The Definitive Guide, Tom White\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to the MapReduce framework\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- `MapReduce`\n",
    "- Jobs\n",
    "- Tasks\n",
    "- Map operation\n",
    "- Reduce operation\n",
    "- Shuffle operation\n",
    "\n",
    "---\n",
    "\n",
    "**Introducing MapReduce**\n",
    "\n",
    "`MapReduce` is a programming framework for processing large data sets with a parallel, distributed algorithm on a cluster.  \n",
    "\n",
    "First the data set is “mapped” into a collection of `<key, value>` pairs, and then “reduced” over all pairs with the same key.  We need to discuss what this means.  \n",
    "\n",
    "The operations are surprisingly broad, enabling them to handle a wide range of use cases.\n",
    "\n",
    "The popular minimal example is to do a word count on some text.\n",
    "\n",
    "Initially published in 2004 by employees at Google\n",
    "\n",
    "A `MapReduce` job is a unit of work the client wants to be performed  \n",
    "\n",
    "A job consists of:  \n",
    "\n",
    "- Input data\n",
    "- `MapReduce` program\n",
    "- configs\n",
    "\n",
    "The job will be divided into *tasks*  \n",
    "Two types of tasks: *map tasks* and *reduce tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Aside on `Hadoop`**  \n",
    "\n",
    "`MapReduce` forms the computation paradigm for Hadoop  \n",
    "\n",
    "`Hadoop` was created by Doug Cutting and Mike Cafarella.  Version 1 was called Nutch.  \n",
    "\n",
    "Yahoo! was instrumental in providing a dedicated team and resources to turn Hadoop into a system that ran at web scale.\n",
    "\n",
    "Hadoop is developed and maintained by the Apache Software Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on Hadoop and Spark**  \n",
    "\n",
    "Spark does not follow the `MapReduce` framework in the same way that `Hadoop` does, where `Mapper` and `Reducer` functions are detailed explicitly.  This is actually a big advantage of Spark.\n",
    "\n",
    "Spark and Hadoop can be better together.  For example, some teams will use Hadoop data storage (HDFS) with Spark.  That is, a Spark job can read data from HDFS, and write results to HDFS.\n",
    "\n",
    "Reported runtimes:\n",
    "Spark can be as much as 10 times faster than `MapReduce` for batch processing, and up to 100 times faster for in-memory analytics.\n",
    "\n",
    "A large fraction of this course will focus on Spark for various tasks.\n",
    "\n",
    "Next we illustrate the `MapReduce` framework with an example.\n",
    "\n",
    "**MapReduce Word Count Process Diagram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"map_reduce_example1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Things to Notice**\n",
    "\n",
    "**Splitting Step**  \n",
    "A given number of records is assigned to each worker\n",
    "\n",
    "NOTE:  \n",
    "More splits means less processing time per split  \n",
    "However, the overhead of managing splits and `map` task creation starts to dominate total job execution time  \n",
    "\n",
    "**Mapping Step**  \n",
    "Each token (word) is given a value of 1 representing the count.  \n",
    "The token, count forms a `<key, value> pair.`  \n",
    "If the same pair appears $n$ times on a worker, there would be $n$ entries of `<map, 1>`.  In other words, no aggregation is done at this step (it happens at the `reduce` step).  \n",
    "\n",
    "**Shuffling Step**  \n",
    "The `<key, value>` pairs are moved across machines at this point, so that pairs with the same key are gathered on the same machine.  This is a costly step in the process.\n",
    "\n",
    "**Reducing Step**  \n",
    "A `reducer` applies a given operation over the values by key.  For this word count example, the counts would be summed for each key and stored with the key.\n",
    "\n",
    "**Map and Reduce in General**  \n",
    "\n",
    "**Map**  \n",
    "Higher-order function that applies a function to each element of a list (e.g., $x => x**2$)\n",
    "\n",
    "**Reduce**  \n",
    "Higher-order function that applies a combining operation (e.g,. cumulative sum of values)\n",
    "\n",
    "**Combiner Functions**  \n",
    "It can be expensive to shuffle data within the cluster, and the limiting factor is the bandwidth on the cluster.\n",
    "\n",
    "`Combiner Functions` can be run on the map output, before the shuffle occurs. This acts locally on each node, and reduces some of the data transfer.  Note this won’t always work (e.g., can’t be used for computing the average, but CAN be used for computing the maximum).\n",
    "\n",
    "\n",
    "**Word Count Example**  \n",
    "Data source: Spark README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "lines = sc.textFile(\"README.md\")\n",
    "lines.take(5)\n",
    "\n",
    "['# Apache Spark', '', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a']\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split())\n",
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)\n",
    "wordcounts.take(10)\n",
    "\n",
    "[(24, 'the'), (17, 'to'), (16, 'Spark'), (12, 'for'), (9, 'and'), (9, '##'), (8, 'a'), (7, 'run'), (7, 'can'), (7, 'on')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How are `map()` and `flatMap()` different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we consider examples of `map` and `flatMap`\n",
    "\n",
    "data = sc.parallelize([3,4,5])\n",
    "\n",
    "type(data)\n",
    "<class 'pyspark.rdd.RDD'>\n",
    "\n",
    "\n",
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() \n",
    "**Output:**\n",
    "[[3, 9], [4, 16], [5, 25]]\n",
    "\n",
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() \n",
    "Output: notice flattened list\n",
    "[3, 9, 4, 16, 5, 25]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
