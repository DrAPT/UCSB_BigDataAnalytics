{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib\n",
    "## *Introduction and Feature Extraction*\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235  \n",
    "### Last Updated: Dec 12, 2018\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "1. Learning Spark\n",
    "2. Spark Documentation  \n",
    "\thttps://spark.apache.org/docs/latest/mllib-data-types.html  \n",
    "\thttp://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Introduction to the machine learning library\n",
    "2. Introduction to MLlib data types\n",
    "3. Discuss Feature Extraction tools in MLLib\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- pipeline  \n",
    "- supervised and unsupervised learning  \n",
    "- learning tasks: classification, regression, clustering, dimensionality reduction  \n",
    "- training set, testing set  \n",
    "- feature extraction  \n",
    "\n",
    "- MLlib data types:  \n",
    "  - LabeledPoint  \n",
    "  - sparse vector, dense vector  \n",
    "  - sparse matrix, dense matrix  \n",
    "  - Rating  \n",
    "\n",
    "- Feature Extraction  \n",
    "- TF-IDF  \n",
    "- Word2Vec  \n",
    "- Cosine Similarity  \n",
    "\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLlib**\n",
    "\n",
    "Contains Sparkâ€™s ML library  \n",
    "Works on RDDs  \n",
    "Contains only algorithms that can be parallelized, since those run well on clusters  \n",
    "Includes a pipeline API useful for building ML pipelines, similar to scikit-learn  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LogReg Classifier to Predict Spam vs Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jovyan/UCSB_BigDataAnalytics/data/mllib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5b2d7f2b6613:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mllib_classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe68eae7668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = sc.textFile(os.path.join(data_path, \"spam.txt\"))\n",
    "ham = sc.textFile(os.path.join(data_path, \"ham.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LabeledPoint datasets (1=spam, 0=ham)\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = positiveExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = negativeExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training set\n",
    "trainData = positiveExamples.union(negativeExamples)\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LogReg model\n",
    "model = LogisticRegressionWithSGD.train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push \"not spam\" example through classifier\n",
    "posTest = tf.transform(\"I love learning Spark programming\".split(\" \"))\n",
    "posTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for positive example: 0\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "print(\"Prediction for positive example: {}\".format(model.predict(posTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabeledPoint**  \n",
    "Stores feature vector together with label  \n",
    "**Rating**  \n",
    "Rating of product by a user. Used in recommendation, for instance.  \n",
    "**Vector**  \n",
    "Handles dense and sparse. For sparse, only nonzero values and their indices are stored.  \n",
    "Sparse saves on memory and runtime.  \n",
    "**Matrix**  \n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single   machine.  \n",
    "MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order.  \n",
    "**Distributed matrix**  \n",
    "A distributed matrix has long-typed row and column indices and double-typed values  \n",
    "**Row matrix**  \n",
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices  \n",
    "**CoordinateMatrix**  \n",
    "CoordinateMatrix is a distributed matrix backed by an RDD of its entries  \n",
    "A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Row-_and_column-major_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sparse vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# create sparse vector [1.0 0.0 2.0 0.0]\n",
    "sv1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "*mllib.feature*  \n",
    "contains classes for common feature transformations:  \n",
    "-  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "Produces feature vectors from text documents\n",
    "\n",
    "There are two algorithms that compute TF-IDF:  \n",
    "\n",
    "**1. HashingTF**  \n",
    "\tComputes term frequency vector from document  \n",
    "\tCan process one document or an RDD of documents  \n",
    "\tEach document needs to be an interable sequence (a list in Python)  \n",
    "\n",
    "To reduce the chance of collision, we can increase the target feature dimension, i.e., the  \n",
    "\t number of buckets of the hash table. The default feature dimension is 1,048,576  \n",
    "\n",
    "**2. IDF**  \n",
    "\tComputes inverse document frequency  \n",
    "\tTerms that appear in high fraction of the docs are not as valuable  \n",
    "\tIDF will downweight such terms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good example of Feature Extraction here:  \n",
    "http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**  \n",
    "Computes distributed vector representation of words.  \n",
    "Similar words are close in the vector space  \n",
    "Useful in many NLP applications:  \n",
    "named entity recognition, disambiguation, parsing, tagging and machine translation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fit Word2VecModel to some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = sc.textFile(\"C:/spark/spark-2.1.1-bin-hadoop2.7/data/text8_part1.txt\").map(lambda row: row.split(\" \"))\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('china', 40)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))\n",
    "Top Records:\n",
    "malaysia: 0.9055396899188917\n",
    "eastern: 0.8834685956632131\n",
    "africa: 0.8537198739068056\n",
    "zambia: 0.8535407384161012\n",
    "myanmar: 0.8475548784366893\n",
    "predominantly: 0.8461926971224027\n",
    "mongolia: 0.8371518611342739\n",
    "countries: 0.8342705781501009\n",
    "southeast: 0.8316274754770454\n",
    "central: 0.8313670331856243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler**   \n",
    "\n",
    "Standardization can improve the convergence rate during the optimization process, and also  \n",
    "prevents against features with very large variances exerting an overly large influence during model   training.  \n",
    "\n",
    "For each feature,  \n",
    "1. Scales to unit variance  \n",
    "2. Centers to mean zero  \n",
    "Useful or even essential for some models  \n",
    "K-means works in Euclidean space, so all features should be on same scale  \n",
    "Tree models do not need this\n",
    "\n",
    "Use this in a *Pipeline* so the statistics can be applied to datasets for scoring later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler  \n",
    "Load dataset in libsvm format, standardize the features so that the new features have unit variance and/or zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MLUtils.loadLibSVMFile(sc, os.path.join(data_path, \"sample_libsvm_data.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.take(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
