{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib\n",
    "## *Introduction and Feature Extraction*\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235  \n",
    "### Last Updated: Dec 12, 2018\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "1. Learning Spark\n",
    "2. Spark Documentation  \n",
    "\thttps://spark.apache.org/docs/latest/mllib-data-types.html  \n",
    "\thttp://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Introduction to the machine learning library\n",
    "2. Introduction to MLlib data types\n",
    "3. Discuss Feature Extraction tools in MLLib\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- pipeline  \n",
    "- supervised and unsupervised learning  \n",
    "- learning tasks: classification, regression, clustering, dimensionality reduction  \n",
    "- training set, testing set  \n",
    "- feature extraction  \n",
    "\n",
    "- MLlib data types:  \n",
    "  - LabeledPoint  \n",
    "  - sparse vector, dense vector  \n",
    "  - sparse matrix, dense matrix  \n",
    "  - Rating  \n",
    "\n",
    "- Feature Extraction  \n",
    "- TF-IDF  \n",
    "- Word2Vec  \n",
    "- Cosine Similarity  \n",
    "\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLlib**\n",
    "\n",
    "Contains Sparkâ€™s ML library  \n",
    "Works on RDDs  \n",
    "Contains only algorithms that can be parallelized, since those run well on clusters  \n",
    "Includes a pipeline API useful for building ML pipelines, similar to scikit-learn  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LogReg Classifier to Predict Spam vs Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jovyan/UCSB_BigDataAnalytics/data/mllib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-adamtashman314:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mllib_classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdf92d40d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = sc.textFile(os.path.join(data_path, \"spam.txt\"))\n",
    "ham = sc.textFile(os.path.join(data_path, \"ham.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...',\n",
       " 'Get Viagra real cheap!  Send money right away to ...',\n",
       " 'Oh my gosh you can be really strong too with these drugs found in the rainforest. Get them cheap right now ...',\n",
       " 'YOUR COMPUTER HAS BEEN INFECTED!  YOU MUST RESET YOUR PASSWORD.  Reply to this email with your password and SSN ...',\n",
       " 'THIS IS NOT A SCAM!  Send money and get access to awesome stuff really cheap and never have to ...']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear Spark Learner, Thanks so much for attending the Spark Summit 2014!  Check out videos of talks from the summit at ...',\n",
       " 'Hi Mom, Apologies for being late about emailing and forgetting to send you the package.  I hope you and bro have been ...',\n",
       " 'Wow, hey Fred, just heard about the Spark petabyte sort.  I think we need to take time to try it out immediately ...',\n",
       " 'Hi Spark user list, This is my first question to this list, so thanks in advance for your help!  I tried running ...',\n",
       " \"Thanks Tom for your email.  I need to refer you to Alice for this one.  I haven't yet figured out that part either ...\",\n",
       " 'Good job yesterday!  I was attending your talk, and really enjoyed it.  I want to try out GraphX ...',\n",
       " 'Summit demo got whoops from audience!  Had to let you know. --Joe']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LabeledPoint datasets (1=spam, 0=ham)\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = positiveExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (10000,[0,365,455,509,1320,1363,1583,2321,2403,3289,3342,4995,5336,5706,5831,6052,6300,6582,6744,8971,8977,9232,9604,9646,9878],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = negativeExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, (10000,[0,1162,2403,2809,3080,3317,4161,4770,5423,5651,5743,5831,6006,6827,6971,7069,7872,9150,9370,9521,9604],[1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[6] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build training set\n",
    "trainData = positiveExamples.union(negativeExamples)\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LogReg model\n",
    "model = LogisticRegressionWithSGD.train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {1205: 1.0, 1605: 1.0, 3682: 1.0, 5195: 1.0, 7279: 2.0, 7779: 1.0, 8321: 2.0, 8517: 1.0, 8550: 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push \"not spam\" example through classifier\n",
    "Test1 = tf.transform(\"I like learning Spark programming more than you love Spark programming\".split(\" \"))\n",
    "Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for example: 0\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "print(\"Prediction for example: {}\".format(model.predict(Test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabeledPoint**  \n",
    "Stores feature vector together with label  \n",
    "**Rating**  \n",
    "Rating of product by a user. Used in recommendation, for instance.  \n",
    "**Vector**  \n",
    "Handles dense and sparse. For sparse, only nonzero values and their indices are stored.  \n",
    "Sparse saves on memory and runtime.  \n",
    "**Matrix**  \n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single   machine.  \n",
    "MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order.  \n",
    "**Distributed matrix**  \n",
    "A distributed matrix has long-typed row and column indices and double-typed values  \n",
    "**Row matrix**  \n",
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices  \n",
    "**CoordinateMatrix**  \n",
    "CoordinateMatrix is a distributed matrix backed by an RDD of its entries  \n",
    "A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Row-_and_column-major_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sparse vector\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# create sparse vector [1.0 0.0 2.0 0.0]\n",
    "sv1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 1.0, 2: 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 2.0, 0.0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.dense(sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "*mllib.feature*  \n",
    "contains classes for common feature transformations:  \n",
    "-  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "Produces feature vectors from text documents\n",
    "\n",
    "There are two algorithms that compute TF-IDF:  \n",
    "\n",
    "**1. HashingTF**  \n",
    "\tComputes term frequency vector from document  \n",
    "\tCan process one document or an RDD of documents  \n",
    "\tEach document needs to be an interable sequence (a list in Python)  \n",
    "\n",
    "To reduce the chance of collision, we can increase the target feature dimension, i.e., the  \n",
    "\t number of buckets of the hash table. The default feature dimension is 1,048,576  \n",
    "\n",
    "**2. IDF**  \n",
    "\tComputes inverse document frequency  \n",
    "\tTerms that appear in high fraction of the docs are not as valuable  \n",
    "\tIDF will downweight such terms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good example of Feature Extraction here:  \n",
    "http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**  \n",
    "Computes distributed vector representation of words.  \n",
    "Similar words are close in the vector space  \n",
    "Useful in many NLP applications:  \n",
    "named entity recognition, disambiguation, parsing, tagging and machine translation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fit Word2VecModel to some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = sc.textFile(\"C:/spark/spark-2.1.1-bin-hadoop2.7/data/text8_part1.txt\").map(lambda row: row.split(\" \"))\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('china', 40)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))\n",
    "Top Records:\n",
    "malaysia: 0.9055396899188917\n",
    "eastern: 0.8834685956632131\n",
    "africa: 0.8537198739068056\n",
    "zambia: 0.8535407384161012\n",
    "myanmar: 0.8475548784366893\n",
    "predominantly: 0.8461926971224027\n",
    "mongolia: 0.8371518611342739\n",
    "countries: 0.8342705781501009\n",
    "southeast: 0.8316274754770454\n",
    "central: 0.8313670331856243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler**   \n",
    "\n",
    "Standardization can improve the convergence rate during the optimization process, and also  \n",
    "prevents against features with very large variances exerting an overly large influence during model   training.  \n",
    "\n",
    "For each feature,  \n",
    "1. Scales to unit variance  \n",
    "2. Centers to mean zero  \n",
    "Useful or even essential for some models  \n",
    "K-means works in Euclidean space, so all features should be on same scale  \n",
    "Tree models do not need this\n",
    "\n",
    "Use this in a *Pipeline* so the statistics can be applied to datasets for scoring later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler  \n",
    "Load dataset in libsvm format, standardize the features so that the new features have unit variance and/or zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MLUtils.loadLibSVMFile(sc, os.path.join(data_path, \"sample_libsvm_data.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = data.map(lambda x: (x.features, x.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}),\n",
       "  0.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.feature.StandardScalerModel"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.feature.StandardScalerModel at 0x7fdfaa425b00>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,\n",
       "  SparseVector(692, {127: 0.5468, 128: 1.5923, 129: 2.4354, 130: 1.7081, 131: 0.7335, 154: 0.4346, 155: 2.0985, 156: 2.2563, 157: 2.2368, 158: 2.2269, 159: 2.2555, 181: 0.4713, 182: 2.0575, 183: 2.3318, 184: 2.3761, 185: 2.1237, 186: 2.0452, 187: 2.2657, 188: 0.6339, 189: 0.1022, 207: 0.1056, 208: 0.5395, 209: 1.9268, 210: 2.2383, 211: 2.3018, 212: 2.3568, 213: 1.8002, 214: 0.7116, 215: 2.2256, 216: 2.4032, 217: 1.5931, 235: 1.5394, 236: 2.188, 237: 2.1493, 238: 2.2924, 239: 2.3889, 240: 2.3155, 241: 2.2653, 242: 0.8445, 243: 1.7094, 244: 2.2496, 245: 1.8613, 262: 0.5062, 263: 2.0796, 264: 2.2201, 265: 2.199, 266: 1.7299, 267: 1.083, 268: 2.1786, 269: 2.0345, 270: 0.4392, 271: 0.7218, 272: 2.2177, 273: 1.6764, 289: 0.4794, 290: 2.214, 291: 2.3569, 292: 2.2283, 293: 1.6322, 294: 0.1087, 295: 0.6833, 296: 1.0411, 297: 0.1941, 300: 2.277, 301: 2.3083, 302: 0.5395, 316: 0.3967, 317: 1.6059, 318: 2.3539, 319: 2.1535, 320: 1.9834, 321: 0.8017, 328: 2.4941, 329: 2.3661, 330: 1.7473, 343: 0.0763, 344: 1.7606, 345: 2.3044, 346: 2.2526, 347: 0.7221, 348: 0.2063, 349: 0.2749, 356: 2.5746, 357: 2.4037, 358: 1.9606, 371: 0.591, 372: 2.4489, 373: 2.3315, 374: 0.6414, 384: 2.5939, 385: 2.4196, 386: 1.8986, 399: 2.0016, 400: 2.3333, 401: 1.898, 412: 2.6198, 413: 2.4525, 414: 1.9856, 426: 0.783, 427: 2.3838, 428: 2.2811, 429: 1.1435, 440: 2.5745, 441: 2.4723, 442: 1.5716, 454: 0.8453, 455: 2.3533, 456: 2.1201, 457: 0.2628, 466: 0.0965, 467: 1.4473, 468: 2.5738, 469: 1.8242, 470: 0.1367, 482: 0.828, 483: 2.3418, 484: 2.0701, 493: 0.1335, 494: 1.4911, 495: 2.4922, 496: 2.224, 497: 0.7444, 510: 0.8429, 511: 2.314, 512: 1.3481, 520: 0.6477, 521: 1.9663, 522: 2.3879, 523: 1.6775, 538: 0.8901, 539: 2.3049, 540: 2.0206, 547: 1.1411, 548: 2.5098, 549: 2.3552, 550: 1.5115, 566: 1.0072, 567: 2.4162, 568: 2.2029, 569: 1.2616, 570: 0.4172, 571: 0.248, 572: 0.8054, 573: 1.6634, 574: 2.0511, 575: 2.349, 576: 2.0094, 577: 1.4659, 578: 0.5412, 594: 1.4063, 595: 2.5988, 596: 2.2365, 597: 2.175, 598: 1.9974, 599: 1.9218, 600: 2.3287, 601: 2.2914, 602: 2.2011, 603: 1.6439, 604: 1.1151, 622: 0.6799, 623: 2.7625, 624: 2.4617, 625: 2.2485, 626: 2.208, 627: 2.323, 628: 2.306, 629: 2.1166, 630: 1.2687, 651: 0.5922, 652: 1.7772, 653: 2.3485, 654: 2.1973, 655: 2.25, 656: 1.2522, 657: 0.3419})),\n",
       " (1.0,\n",
       "  SparseVector(692, {158: 1.0958, 159: 2.4077, 160: 3.3846, 161: 1.7204, 185: 0.853, 186: 2.1418, 187: 2.2567, 188: 2.8135, 189: 1.0559, 213: 1.1318, 214: 2.1262, 215: 2.2168, 216: 2.4032, 217: 0.8096, 240: 0.6248, 241: 2.1215, 242: 2.208, 243: 1.9084, 244: 0.2756, 245: 0.0892, 267: 0.57, 268: 1.9634, 269: 2.2397, 270: 2.3457, 271: 0.8589, 295: 1.4121, 296: 2.1768, 297: 2.3387, 298: 2.0953, 322: 0.1786, 323: 2.2172, 324: 2.2457, 325: 2.3935, 326: 0.8686, 349: 0.3142, 350: 1.7704, 351: 2.1356, 352: 2.3335, 353: 1.5228, 377: 1.0105, 378: 2.0366, 379: 2.1317, 380: 1.837, 381: 0.2433, 404: 1.1071, 405: 2.0948, 406: 2.0187, 407: 1.7035, 408: 0.2743, 431: 0.6566, 432: 2.8815, 433: 2.1092, 434: 2.0364, 435: 1.4977, 459: 2.3019, 460: 2.4603, 461: 2.0737, 462: 2.0814, 463: 0.3887, 486: 0.6592, 487: 2.4874, 488: 2.2454, 489: 2.0844, 490: 1.5147, 514: 2.8366, 515: 2.4273, 516: 2.2193, 517: 1.7178, 518: 0.1059, 542: 2.3486, 543: 2.2537, 544: 2.2517, 545: 0.8152, 569: 1.3739, 570: 2.2164, 571: 2.1633, 572: 2.3972, 573: 0.2897, 596: 0.426, 597: 1.9679, 598: 2.2067, 599: 2.2079, 600: 1.2937, 601: 0.0727, 624: 0.6252, 625: 2.2396, 626: 2.208, 627: 2.028, 652: 0.8886, 653: 2.3392, 654: 2.1973, 655: 1.9643, 680: 0.674, 681: 3.2075, 682: 3.5511, 683: 2.9512}))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
