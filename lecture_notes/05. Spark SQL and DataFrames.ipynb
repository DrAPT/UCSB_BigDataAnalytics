{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235: Big Data Analytics\n",
    "### Last Updated: January 29, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning Spark, Chapter 9: Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "Demonstration of several useful DataFrame operations:  \n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to Spark SQL, the interface for working with structured and semistructured data\n",
    "- Introduce DataFrames and show basic functionality\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Schema\n",
    "- SQL\n",
    "- Dataset and DataFrame\n",
    "- Partition\n",
    "- Parquet files\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "These lecture notes are a quick outline of Spark SQL and DataFrames.  \n",
    "There is a lot of functionality provided, and Spark SQL and DataFrames are relatively new.  \n",
    "The DataFrame has replaced the SchemaRDD\n",
    "\n",
    "**Spark SQL Basics**\n",
    "\n",
    "A database *schema* is the structure that represents the logical view of the entire database.   \n",
    "Defines how data is organized and how relations among them are associated  \n",
    "Defines tables, views, integrity constraints\n",
    "\n",
    "SQL is a structured query language used to communicate with relational databases.  \n",
    "Commands include CREATE, SELECT, UPDATE, ALTER, INSERT INTO, DROP, DELETE\n",
    "\n",
    "***Spark SQL Capabilities:***\n",
    "\n",
    "1. Can load data from various structured formats including JSON, Hive, Parquet  \n",
    "2. Can query data using SQL inside Spark or from external tools that connect to Spark (e.g., Tableau) \n",
    "3. Spark SQL integrates between SQL and Python/Java/Scala/R code. Can do things like join RDDs and SQL tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset and DataFrame**\n",
    "\n",
    "- A Dataset is a distributed collection of data.   \n",
    "- A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).  \n",
    "- A DataFrame is a Dataset organized into named columns.   \n",
    "Think of a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.   \n",
    "\n",
    "- DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.  \n",
    "\n",
    "- The DataFrame API is available in Scala, Java, Python, and R. \n",
    "\n",
    "**DataFrames vs RDDs**  \n",
    "Use RDDs to perform low-level transformations and actions on unstructured data. \n",
    "\n",
    "This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. \n",
    "\n",
    "Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
    "\n",
    "Use DataFrames to use high-level expressions, to perform SQL queries to explore the data, and to gain columnar access\n",
    " \n",
    "**Creating a DataFrame from an RDD**  \n",
    "The following example illustrates the conversion from an RDD to a DataFrame, where we impose a schema on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a DataFrame from some JSON data**  \n",
    "(For an example of JSON data see: http://json.org/example.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "# +----+-------+\n",
    "df.columns\n",
    "['age', 'name']\n",
    "df.count()\n",
    "3\n",
    "# Take first 2 rows\n",
    "dfh = df.limit(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we turn to the documentation to explore more DataFrame functionality including \n",
    "Subsetting, filtering, aggregation\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "filterDF = df.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch records w first name null or last name null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterNonNullDF = DF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where() is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whereDF = DF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")) \\\n",
    "            .sort(asc(\"lastName\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nonNullDF = DF.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the salary field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF.describe(“salary”).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from a registered table (e.g., Hive metastore) into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = spark.sql(\"select * from sample_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Provide the min, count, and avg and groupBy the location column\n",
    "agg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DF to Parquet file, partitioning on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\n",
    "display(dbutils.fs.ls(\"/tmp/sample_table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer the schema when reading in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adult_df = spark.read.\\\n",
    "    format(\"com.spark.csv\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\n",
    "\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Temporary View\n",
    "It is possible to register a DataFrame as a SQL temporary view, and then query the view writing straight SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT FROM people where name=='Andy'\")\n",
    "sqlDF.show()\n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load using Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load using Manually Specified Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files\n",
    "\n",
    "- Project was developed at Twitter, taken over by Apache Software Foundation (Apache)   \n",
    "- Parquet is a columnar format that is supported by many other data processing systems  \n",
    "\n",
    "- Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observation: It can be much more efficient to store data in terms of columns than rows.  \n",
    "Column data is stored in contiguous memory blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load Operations using Parquet Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data in JSON format. This will produce a DataFrame.\n",
    "peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# Loading parquet files produces a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database tables can be partitioned to make querying more efficient.  \n",
    "For example, the data can be\n",
    "split by gender and country, producing smaller tables.  \n",
    "If the analyst is only interested in a single country, the query will run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory.  \n",
    "\n",
    "All built-in file sources (including Text/CSV/JSON/ORC/Parquet) are able to discover and infer partitioning information automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── ...\n",
    "        │   │\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        │   └── ...\n",
    "        └── gender=female\n",
    "            ├── ...\n",
    "            │\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "            └── ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
