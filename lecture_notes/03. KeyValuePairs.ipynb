{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Value Pairs\n",
    "\n",
    "### University of California, Santa Barbara  \n",
    "### PSTAT 135/235: Big Data Analytics\n",
    "### Last Updated: January 29, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "1. Learning Spark\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Learn about properties and methods for pair RDDs\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Pair RDDs  \n",
    "- Partition  \n",
    "- reduceByKey(), groupByKey(), combineByKey(), sortByKey()  \n",
    "- mapValues(), flatMapValues()  \n",
    "- keys(), values()  \n",
    "- join(), subtractByKey(), rightOuterJoin(), leftOuterJoin(), cogroup  \n",
    "- countByKey()  \n",
    "- collectAsMap()  \n",
    "- lookup()  \n",
    "- groupWith()  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIR RDD BASICS\n",
    "\n",
    "A pair RDD contains key/value pairs (e.g., dictionary in Python)  \n",
    "\n",
    "Useful for merging, aggregating  \n",
    "\n",
    "Calling map() on an RDD will produce a pair RDD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['french fries','chicken burrito'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['french', 'fries'], ['chicken', 'burrito']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['french', 'fries', 'chicken', 'burrito']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['french', 'french fries', 'chicken', 'chicken burrito']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(lambda x: (x.split(\" \")[0], x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('french', 'french fries'), ('chicken', 'chicken burrito')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map(lambda x: (x.split(\" \")[0], x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lines.flatMap(lambda x: (x.split(\" \")[0], x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['french', 'french fries', 'chicken', 'chicken burrito']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['french', 'french fries', 'chicken', 'chicken burrito']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in p if 'french' in p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 49 contains transformations on pair RDDs  \n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2),(3,4),(3,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the keys\n",
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Transformations**\n",
    "\n",
    "fold()  \n",
    "Similar to reduce, includes “zero value” acting as identity\n",
    "\n",
    "reduceByKey()  \n",
    "Runs several parallel reduce operations, one for each key  \n",
    "Combining is done locally on each machine for each key before computing a global combine for the key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce (sum) by keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y: x+y) \\\n",
    "   .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisiting Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('/home/jovyan/UCSB_BigDataAnalytics/data/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# apache spark',\n",
       " '',\n",
       " 'spark is a fast and general cluster computing system for big data    it provides',\n",
       " 'high level apis in scala  java  python  and r  and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis    it also supports a']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower())\n",
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('apache', 1), ('spark', 1), ('spark', 1), ('is', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower()) \\\n",
    "                        .flatMap(lambda x: x.split()) \\\n",
    "                        .map(lambda x: (x, 1))\n",
    "\n",
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('apache', 11), ('spark', 19), ('is', 6), ('a', 9)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower()) \\\n",
    "                        .flatMap(lambda x: x.split()) \\\n",
    "                        .map(lambda x: (x, 1)) \\\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 'apache')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower()) \\\n",
    "                        .flatMap(lambda x: x.split()) \\\n",
    "                        .filter(lambda x: x == 'apache') \\\n",
    "                        .map(lambda x: (x, 1)) \\\n",
    "                        .reduceByKey(lambda x,y:x+y) \\\n",
    "                        .map(lambda x:(x[1],x[0])) \\\n",
    "                        .sortByKey(False)\n",
    "wordcounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower()) \\\n",
    "                        .flatMap(lambda x: x.split()) \\\n",
    "                        .map(lambda x: (x, 1)) \\\n",
    "                        .reduceByKey(lambda x,y:x+y) \\\n",
    "                        .map(lambda x:(x[1],x[0])) \\\n",
    "                        .sortByKey(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26, 'the'),\n",
       " (19, 'spark'),\n",
       " (19, 'to'),\n",
       " (15, 'for'),\n",
       " (11, 'apache'),\n",
       " (10, 'and'),\n",
       " (9, 'a'),\n",
       " (9, '##'),\n",
       " (8, 'you'),\n",
       " (7, 'can')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 'Apache', 'Spark'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'and',\n",
       "  'general',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'system',\n",
       "  'for',\n",
       "  'Big',\n",
       "  'Data.',\n",
       "  'It',\n",
       "  'provides'],\n",
       " ['high-level',\n",
       "  'APIs',\n",
       "  'in',\n",
       "  'Scala,',\n",
       "  'Java,',\n",
       "  'Python,',\n",
       "  'and',\n",
       "  'R,',\n",
       "  'and',\n",
       "  'an',\n",
       "  'optimized',\n",
       "  'engine',\n",
       "  'that'],\n",
       " ['supports',\n",
       "  'general',\n",
       "  'computation',\n",
       "  'graphs',\n",
       "  'for',\n",
       "  'data',\n",
       "  'analysis.',\n",
       "  'It',\n",
       "  'also',\n",
       "  'supports',\n",
       "  'a']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = lines \\\n",
    "            .map(lambda x: x.split())\n",
    "bigrams.take(5)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines2 = sc.parallelize(lines.take(1))\n",
    "lines2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 'Apache'), ('Apache', 'Spark')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = lines2 \\\n",
    "            .map(lambda x: x.split()) \\\n",
    "            .flatMap(lambda x: [(x[i],x[i+1]) for i in range(0,len(x)-1)])\n",
    "bigrams.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('#', 'Apache'), 1),\n",
       " (('Apache', 'Spark'), 1),\n",
       " (('Spark', 'is'), 1),\n",
       " (('is', 'a'), 1),\n",
       " (('a', 'fast'), 1)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = lines \\\n",
    "            .map(lambda x: x.split()) \\\n",
    "            .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "bigrams.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = lines \\\n",
    "            .map(lambda x: x.split()) \\\n",
    "            .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('#', 'Apache'), 1),\n",
       " (('Apache', 'Spark'), 1),\n",
       " (('Spark', 'is'), 1),\n",
       " (('is', 'a'), 1),\n",
       " (('a', 'fast'), 1),\n",
       " (('fast', 'and'), 1),\n",
       " (('and', 'general'), 1),\n",
       " (('general', 'cluster'), 1),\n",
       " (('cluster', 'computing'), 1),\n",
       " (('computing', 'system'), 1),\n",
       " (('system', 'for'), 1),\n",
       " (('for', 'Big'), 1),\n",
       " (('Big', 'Data.'), 1),\n",
       " (('Data.', 'It'), 1),\n",
       " (('It', 'provides'), 1),\n",
       " (('high-level', 'APIs'), 1),\n",
       " (('APIs', 'in'), 1),\n",
       " (('in', 'Scala,'), 1),\n",
       " (('Scala,', 'Java,'), 1),\n",
       " (('Java,', 'Python,'), 1),\n",
       " (('Python,', 'and'), 1),\n",
       " (('and', 'R,'), 1),\n",
       " (('R,', 'and'), 1),\n",
       " (('and', 'an'), 1),\n",
       " (('an', 'optimized'), 1),\n",
       " (('optimized', 'engine'), 1),\n",
       " (('engine', 'that'), 1),\n",
       " (('supports', 'general'), 1),\n",
       " (('general', 'computation'), 1),\n",
       " (('computation', 'graphs'), 1),\n",
       " (('graphs', 'for'), 1),\n",
       " (('for', 'data'), 1),\n",
       " (('data', 'analysis.'), 1),\n",
       " (('analysis.', 'It'), 1),\n",
       " (('It', 'also'), 1),\n",
       " (('also', 'supports'), 1),\n",
       " (('supports', 'a'), 1),\n",
       " (('rich', 'set'), 1),\n",
       " (('set', 'of'), 1),\n",
       " (('of', 'higher-level'), 1),\n",
       " (('higher-level', 'tools'), 1),\n",
       " (('tools', 'including'), 1),\n",
       " (('including', 'Spark'), 1),\n",
       " (('Spark', 'SQL'), 1),\n",
       " (('SQL', 'for'), 1),\n",
       " (('for', 'SQL'), 1),\n",
       " (('SQL', 'and'), 1),\n",
       " (('and', 'DataFrames,'), 1),\n",
       " (('MLlib', 'for'), 1),\n",
       " (('for', 'machine'), 1),\n",
       " (('machine', 'learning,'), 1),\n",
       " (('learning,', 'GraphX'), 1),\n",
       " (('GraphX', 'for'), 1),\n",
       " (('for', 'graph'), 1),\n",
       " (('graph', 'processing,'), 1),\n",
       " (('and', 'Spark'), 1),\n",
       " (('Spark', 'Streaming'), 1),\n",
       " (('Streaming', 'for'), 1),\n",
       " (('for', 'stream'), 1),\n",
       " (('stream', 'processing.'), 1),\n",
       " (('##', 'Online'), 1),\n",
       " (('Online', 'Documentation'), 1),\n",
       " (('You', 'can'), 1),\n",
       " (('can', 'find'), 1),\n",
       " (('find', 'the'), 1),\n",
       " (('the', 'latest'), 1),\n",
       " (('latest', 'Spark'), 1),\n",
       " (('Spark', 'documentation,'), 1),\n",
       " (('documentation,', 'including'), 1),\n",
       " (('including', 'a'), 1),\n",
       " (('a', 'programming'), 1),\n",
       " (('guide,', 'on'), 1),\n",
       " (('on', 'the'), 1),\n",
       " (('the', '[project'), 1),\n",
       " (('[project', 'web'), 1),\n",
       " (('web', 'page](http://spark.apache.org/documentation.html).'), 1),\n",
       " (('This', 'README'), 1),\n",
       " (('README', 'file'), 1),\n",
       " (('file', 'only'), 1),\n",
       " (('only', 'contains'), 1),\n",
       " (('contains', 'basic'), 1),\n",
       " (('basic', 'setup'), 1),\n",
       " (('setup', 'instructions.'), 1),\n",
       " (('##', 'Building'), 1),\n",
       " (('Building', 'Spark'), 1),\n",
       " (('Spark', 'is'), 1),\n",
       " (('is', 'built'), 1),\n",
       " (('built', 'using'), 1),\n",
       " (('using', '[Apache'), 1),\n",
       " (('[Apache', 'Maven](http://maven.apache.org/).'), 1),\n",
       " (('To', 'build'), 1),\n",
       " (('build', 'Spark'), 1),\n",
       " (('Spark', 'and'), 1),\n",
       " (('and', 'its'), 1),\n",
       " (('its', 'example'), 1),\n",
       " (('example', 'programs,'), 1),\n",
       " (('programs,', 'run:'), 1),\n",
       " (('build/mvn', '-DskipTests'), 1),\n",
       " (('-DskipTests', 'clean'), 1),\n",
       " (('clean', 'package'), 1),\n",
       " (('(You', 'do'), 1),\n",
       " (('do', 'not'), 1),\n",
       " (('not', 'need'), 1),\n",
       " (('need', 'to'), 1),\n",
       " (('to', 'do'), 1),\n",
       " (('do', 'this'), 1),\n",
       " (('this', 'if'), 1),\n",
       " (('if', 'you'), 1),\n",
       " (('you', 'downloaded'), 1),\n",
       " (('downloaded', 'a'), 1),\n",
       " (('a', 'pre-built'), 1),\n",
       " (('pre-built', 'package.)'), 1),\n",
       " (('You', 'can'), 1),\n",
       " (('can', 'build'), 1),\n",
       " (('build', 'Spark'), 1),\n",
       " (('Spark', 'using'), 1),\n",
       " (('using', 'more'), 1),\n",
       " (('more', 'than'), 1),\n",
       " (('than', 'one'), 1),\n",
       " (('one', 'thread'), 1),\n",
       " (('thread', 'by'), 1),\n",
       " (('by', 'using'), 1),\n",
       " (('using', 'the'), 1),\n",
       " (('the', '-T'), 1),\n",
       " (('-T', 'option'), 1),\n",
       " (('option', 'with'), 1),\n",
       " (('with', 'Maven,'), 1),\n",
       " (('Maven,', 'see'), 1),\n",
       " (('see', '[\"Parallel'), 1),\n",
       " (('[\"Parallel', 'builds'), 1),\n",
       " (('builds', 'in'), 1),\n",
       " (('in', 'Maven'), 1),\n",
       " (('Maven',\n",
       "   '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'),\n",
       "  1),\n",
       " (('More', 'detailed'), 1),\n",
       " (('detailed', 'documentation'), 1),\n",
       " (('documentation', 'is'), 1),\n",
       " (('is', 'available'), 1),\n",
       " (('available', 'from'), 1),\n",
       " (('from', 'the'), 1),\n",
       " (('the', 'project'), 1),\n",
       " (('project', 'site,'), 1),\n",
       " (('site,', 'at'), 1),\n",
       " (('[\"Building',\n",
       "   'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'),\n",
       "  1),\n",
       " (('For', 'general'), 1),\n",
       " (('general', 'development'), 1),\n",
       " (('development', 'tips,'), 1),\n",
       " (('tips,', 'including'), 1),\n",
       " (('including', 'info'), 1),\n",
       " (('info', 'on'), 1),\n",
       " (('on', 'developing'), 1),\n",
       " (('developing', 'Spark'), 1),\n",
       " (('Spark', 'using'), 1),\n",
       " (('using', 'an'), 1),\n",
       " (('an', 'IDE,'), 1),\n",
       " (('IDE,', 'see'), 1),\n",
       " (('see', '[\"Useful'), 1),\n",
       " (('[\"Useful', 'Developer'), 1),\n",
       " (('Developer', 'Tools\"](http://spark.apache.org/developer-tools.html).'), 1),\n",
       " (('##', 'Interactive'), 1),\n",
       " (('Interactive', 'Scala'), 1),\n",
       " (('Scala', 'Shell'), 1),\n",
       " (('The', 'easiest'), 1),\n",
       " (('easiest', 'way'), 1),\n",
       " (('way', 'to'), 1),\n",
       " (('to', 'start'), 1),\n",
       " (('start', 'using'), 1),\n",
       " (('using', 'Spark'), 1),\n",
       " (('Spark', 'is'), 1),\n",
       " (('is', 'through'), 1),\n",
       " (('through', 'the'), 1),\n",
       " (('the', 'Scala'), 1),\n",
       " (('Scala', 'shell:'), 1),\n",
       " (('Try', 'the'), 1),\n",
       " (('the', 'following'), 1),\n",
       " (('following', 'command,'), 1),\n",
       " (('command,', 'which'), 1),\n",
       " (('which', 'should'), 1),\n",
       " (('should', 'return'), 1),\n",
       " (('return', '1000:'), 1),\n",
       " (('scala>', 'sc.parallelize(1'), 1),\n",
       " (('sc.parallelize(1', 'to'), 1),\n",
       " (('to', '1000).count()'), 1),\n",
       " (('##', 'Interactive'), 1),\n",
       " (('Interactive', 'Python'), 1),\n",
       " (('Python', 'Shell'), 1),\n",
       " (('Alternatively,', 'if'), 1),\n",
       " (('if', 'you'), 1),\n",
       " (('you', 'prefer'), 1),\n",
       " (('prefer', 'Python,'), 1),\n",
       " (('Python,', 'you'), 1),\n",
       " (('you', 'can'), 1),\n",
       " (('can', 'use'), 1),\n",
       " (('use', 'the'), 1),\n",
       " (('the', 'Python'), 1),\n",
       " (('Python', 'shell:'), 1),\n",
       " (('And', 'run'), 1),\n",
       " (('run', 'the'), 1),\n",
       " (('the', 'following'), 1),\n",
       " (('following', 'command,'), 1),\n",
       " (('command,', 'which'), 1),\n",
       " (('which', 'should'), 1),\n",
       " (('should', 'also'), 1),\n",
       " (('also', 'return'), 1),\n",
       " (('return', '1000:'), 1),\n",
       " (('>>>', 'sc.parallelize(range(1000)).count()'), 1),\n",
       " (('##', 'Example'), 1),\n",
       " (('Example', 'Programs'), 1),\n",
       " (('Spark', 'also'), 1),\n",
       " (('also', 'comes'), 1),\n",
       " (('comes', 'with'), 1),\n",
       " (('with', 'several'), 1),\n",
       " (('several', 'sample'), 1),\n",
       " (('sample', 'programs'), 1),\n",
       " (('programs', 'in'), 1),\n",
       " (('in', 'the'), 1),\n",
       " (('the', '`examples`'), 1),\n",
       " (('`examples`', 'directory.'), 1),\n",
       " (('To', 'run'), 1),\n",
       " (('run', 'one'), 1),\n",
       " (('one', 'of'), 1),\n",
       " (('of', 'them,'), 1),\n",
       " (('them,', 'use'), 1),\n",
       " (('use', '`./bin/run-example'), 1),\n",
       " (('`./bin/run-example', '<class>'), 1),\n",
       " (('<class>', '[params]`.'), 1),\n",
       " (('[params]`.', 'For'), 1),\n",
       " (('For', 'example:'), 1),\n",
       " (('./bin/run-example', 'SparkPi'), 1),\n",
       " (('will', 'run'), 1),\n",
       " (('run', 'the'), 1),\n",
       " (('the', 'Pi'), 1),\n",
       " (('Pi', 'example'), 1),\n",
       " (('example', 'locally.'), 1),\n",
       " (('You', 'can'), 1),\n",
       " (('can', 'set'), 1),\n",
       " (('set', 'the'), 1),\n",
       " (('the', 'MASTER'), 1),\n",
       " (('MASTER', 'environment'), 1),\n",
       " (('environment', 'variable'), 1),\n",
       " (('variable', 'when'), 1),\n",
       " (('when', 'running'), 1),\n",
       " (('running', 'examples'), 1),\n",
       " (('examples', 'to'), 1),\n",
       " (('to', 'submit'), 1),\n",
       " (('examples', 'to'), 1),\n",
       " (('to', 'a'), 1),\n",
       " (('a', 'cluster.'), 1),\n",
       " (('cluster.', 'This'), 1),\n",
       " (('This', 'can'), 1),\n",
       " (('can', 'be'), 1),\n",
       " (('be', 'a'), 1),\n",
       " (('a', 'mesos://'), 1),\n",
       " (('mesos://', 'or'), 1),\n",
       " (('or', 'spark://'), 1),\n",
       " (('spark://', 'URL,'), 1),\n",
       " (('\"yarn\"', 'to'), 1),\n",
       " (('to', 'run'), 1),\n",
       " (('run', 'on'), 1),\n",
       " (('on', 'YARN,'), 1),\n",
       " (('YARN,', 'and'), 1),\n",
       " (('and', '\"local\"'), 1),\n",
       " (('\"local\"', 'to'), 1),\n",
       " (('to', 'run'), 1),\n",
       " (('locally', 'with'), 1),\n",
       " (('with', 'one'), 1),\n",
       " (('one', 'thread,'), 1),\n",
       " (('thread,', 'or'), 1),\n",
       " (('or', '\"local[N]\"'), 1),\n",
       " (('\"local[N]\"', 'to'), 1),\n",
       " (('to', 'run'), 1),\n",
       " (('run', 'locally'), 1),\n",
       " (('locally', 'with'), 1),\n",
       " (('with', 'N'), 1),\n",
       " (('N', 'threads.'), 1),\n",
       " (('threads.', 'You'), 1),\n",
       " (('can', 'also'), 1),\n",
       " (('also', 'use'), 1),\n",
       " (('use', 'an'), 1),\n",
       " (('an', 'abbreviated'), 1),\n",
       " (('abbreviated', 'class'), 1),\n",
       " (('class', 'name'), 1),\n",
       " (('name', 'if'), 1),\n",
       " (('if', 'the'), 1),\n",
       " (('the', 'class'), 1),\n",
       " (('class', 'is'), 1),\n",
       " (('is', 'in'), 1),\n",
       " (('in', 'the'), 1),\n",
       " (('the', '`examples`'), 1),\n",
       " (('package.', 'For'), 1),\n",
       " (('For', 'instance:'), 1),\n",
       " (('MASTER=spark://host:7077', './bin/run-example'), 1),\n",
       " (('./bin/run-example', 'SparkPi'), 1),\n",
       " (('Many', 'of'), 1),\n",
       " (('of', 'the'), 1),\n",
       " (('the', 'example'), 1),\n",
       " (('example', 'programs'), 1),\n",
       " (('programs', 'print'), 1),\n",
       " (('print', 'usage'), 1),\n",
       " (('usage', 'help'), 1),\n",
       " (('help', 'if'), 1),\n",
       " (('if', 'no'), 1),\n",
       " (('no', 'params'), 1),\n",
       " (('params', 'are'), 1),\n",
       " (('are', 'given.'), 1),\n",
       " (('##', 'Running'), 1),\n",
       " (('Running', 'Tests'), 1),\n",
       " (('Testing', 'first'), 1),\n",
       " (('first', 'requires'), 1),\n",
       " (('requires', '[building'), 1),\n",
       " (('[building', 'Spark](#building-spark).'), 1),\n",
       " (('Spark](#building-spark).', 'Once'), 1),\n",
       " (('Once', 'Spark'), 1),\n",
       " (('Spark', 'is'), 1),\n",
       " (('is', 'built,'), 1),\n",
       " (('built,', 'tests'), 1),\n",
       " (('can', 'be'), 1),\n",
       " (('be', 'run'), 1),\n",
       " (('run', 'using:'), 1),\n",
       " (('Please', 'see'), 1),\n",
       " (('see', 'the'), 1),\n",
       " (('the', 'guidance'), 1),\n",
       " (('guidance', 'on'), 1),\n",
       " (('on', 'how'), 1),\n",
       " (('how', 'to'), 1),\n",
       " (('[run', 'tests'), 1),\n",
       " (('tests', 'for'), 1),\n",
       " (('for', 'a'), 1),\n",
       " (('a', 'module,'), 1),\n",
       " (('module,', 'or'), 1),\n",
       " (('or', 'individual'), 1),\n",
       " (('individual',\n",
       "   'tests](http://spark.apache.org/developer-tools.html#individual-tests).'),\n",
       "  1),\n",
       " (('##', 'A'), 1),\n",
       " (('A', 'Note'), 1),\n",
       " (('Note', 'About'), 1),\n",
       " (('About', 'Hadoop'), 1),\n",
       " (('Hadoop', 'Versions'), 1),\n",
       " (('Spark', 'uses'), 1),\n",
       " (('uses', 'the'), 1),\n",
       " (('the', 'Hadoop'), 1),\n",
       " (('Hadoop', 'core'), 1),\n",
       " (('core', 'library'), 1),\n",
       " (('library', 'to'), 1),\n",
       " (('to', 'talk'), 1),\n",
       " (('talk', 'to'), 1),\n",
       " (('to', 'HDFS'), 1),\n",
       " (('HDFS', 'and'), 1),\n",
       " (('and', 'other'), 1),\n",
       " (('other', 'Hadoop-supported'), 1),\n",
       " (('storage', 'systems.'), 1),\n",
       " (('systems.', 'Because'), 1),\n",
       " (('Because', 'the'), 1),\n",
       " (('the', 'protocols'), 1),\n",
       " (('protocols', 'have'), 1),\n",
       " (('have', 'changed'), 1),\n",
       " (('changed', 'in'), 1),\n",
       " (('in', 'different'), 1),\n",
       " (('different', 'versions'), 1),\n",
       " (('versions', 'of'), 1),\n",
       " (('Hadoop,', 'you'), 1),\n",
       " (('you', 'must'), 1),\n",
       " (('must', 'build'), 1),\n",
       " (('build', 'Spark'), 1),\n",
       " (('Spark', 'against'), 1),\n",
       " (('against', 'the'), 1),\n",
       " (('the', 'same'), 1),\n",
       " (('same', 'version'), 1),\n",
       " (('version', 'that'), 1),\n",
       " (('that', 'your'), 1),\n",
       " (('your', 'cluster'), 1),\n",
       " (('cluster', 'runs.'), 1),\n",
       " (('Please', 'refer'), 1),\n",
       " (('refer', 'to'), 1),\n",
       " (('to', 'the'), 1),\n",
       " (('the', 'build'), 1),\n",
       " (('build', 'documentation'), 1),\n",
       " (('documentation', 'at'), 1),\n",
       " (('[\"Specifying', 'the'), 1),\n",
       " (('the', 'Hadoop'), 1),\n",
       " (('Hadoop',\n",
       "   'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'),\n",
       "  1),\n",
       " (('for', 'detailed'), 1),\n",
       " (('detailed', 'guidance'), 1),\n",
       " (('guidance', 'on'), 1),\n",
       " (('on', 'building'), 1),\n",
       " (('building', 'for'), 1),\n",
       " (('for', 'a'), 1),\n",
       " (('a', 'particular'), 1),\n",
       " (('particular', 'distribution'), 1),\n",
       " (('distribution', 'of'), 1),\n",
       " (('of', 'Hadoop,'), 1),\n",
       " (('Hadoop,', 'including'), 1),\n",
       " (('building', 'for'), 1),\n",
       " (('for', 'particular'), 1),\n",
       " (('particular', 'Hive'), 1),\n",
       " (('Hive', 'and'), 1),\n",
       " (('and', 'Hive'), 1),\n",
       " (('Hive', 'Thriftserver'), 1),\n",
       " (('Thriftserver', 'distributions.'), 1),\n",
       " (('##', 'Configuration'), 1),\n",
       " (('Please', 'refer'), 1),\n",
       " (('refer', 'to'), 1),\n",
       " (('to', 'the'), 1),\n",
       " (('the', '[Configuration'), 1),\n",
       " (('[Configuration',\n",
       "   'Guide](http://spark.apache.org/docs/latest/configuration.html)'),\n",
       "  1),\n",
       " (('in', 'the'), 1),\n",
       " (('the', 'online'), 1),\n",
       " (('online', 'documentation'), 1),\n",
       " (('documentation', 'for'), 1),\n",
       " (('for', 'an'), 1),\n",
       " (('an', 'overview'), 1),\n",
       " (('overview', 'on'), 1),\n",
       " (('on', 'how'), 1),\n",
       " (('how', 'to'), 1),\n",
       " (('to', 'configure'), 1),\n",
       " (('configure', 'Spark.'), 1),\n",
       " (('##', 'Contributing'), 1),\n",
       " (('Please', 'review'), 1),\n",
       " (('review', 'the'), 1),\n",
       " (('the', '[Contribution'), 1),\n",
       " (('[Contribution', 'to'), 1),\n",
       " (('to', 'Spark'), 1),\n",
       " (('Spark', 'guide](http://spark.apache.org/contributing.html)'), 1),\n",
       " (('for', 'information'), 1),\n",
       " (('information', 'on'), 1),\n",
       " (('on', 'how'), 1),\n",
       " (('how', 'to'), 1),\n",
       " (('to', 'get'), 1),\n",
       " (('get', 'started'), 1),\n",
       " (('started', 'contributing'), 1),\n",
       " (('contributing', 'to'), 1),\n",
       " (('to', 'the'), 1),\n",
       " (('the', 'project.'), 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('#', 'Apache'), 1),\n",
       " (('Apache', 'Spark'), 1),\n",
       " (('Spark', 'is'), 4),\n",
       " (('is', 'a'), 1),\n",
       " (('a', 'fast'), 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = lines \\\n",
    "          .map(lambda x: x.split()) \\\n",
    "          .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\\\n",
    "          .reduceByKey(lambda x,y: x+y)\n",
    "bigrams.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('#', 'Apache')),\n",
       " (1, ('Apache', 'Spark')),\n",
       " (4, ('Spark', 'is')),\n",
       " (1, ('is', 'a')),\n",
       " (1, ('a', 'fast'))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = lines \\\n",
    "          .map(lambda x: x.split()) \\\n",
    "          .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\\\n",
    "          .reduceByKey(lambda x,y: x+y) \\\n",
    "          .map(lambda x: (x[1],x[0]))\n",
    "bigrams.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('Spark', 'is')),\n",
       " (3, ('You', 'can')),\n",
       " (3, ('build', 'Spark')),\n",
       " (3, ('in', 'the')),\n",
       " (3, ('to', 'run')),\n",
       " (3, ('on', 'how')),\n",
       " (3, ('how', 'to')),\n",
       " (3, ('to', 'the')),\n",
       " (2, ('if', 'you')),\n",
       " (2, ('Spark', 'using'))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# including a reducer and sort by key\n",
    "\n",
    "bigrams = lines \\\n",
    "          .map(lambda x: x.split()) \\\n",
    "          .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\\\n",
    "          .reduceByKey(lambda x,y: x+y) \\\n",
    "          .map(lambda x: (x[1],x[0])) \\\n",
    "          .sortByKey(False)\n",
    "bigrams.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition**  \n",
    "\n",
    "Determines the amount of parallelism when executing on RDDs.  \n",
    "Most operators in this chapter take parameter for partition.  \n",
    "\n",
    "Example: reduceByKey(lambda x, y: x + y, 10)\n",
    "\n",
    "**Join**  \n",
    "join()  is an inner join  \n",
    "leftOuterJoin()  \n",
    "rightOuterJoin()  \n",
    "\n",
    "**Sorting**  \n",
    "Takes param for sort direction.  \n",
    "Can provide comparison function for custom sorting.  \n",
    "Example of converting integers to strings and using string compare function:  \n",
    "rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))  \n",
    "\n",
    "**Actions on Pair RDDs**  \n",
    "All transformations for base RDDs are avail for pair RDDs  \n",
    "Plus some additional like:  \n",
    "countByKey()  \n",
    "collectAsMap()  \n",
    "lookup(key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2),(3,4),(3,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.lookup(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
