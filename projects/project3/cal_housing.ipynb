{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University of California, Santa Barbara\n",
    "## PSTAT 135 / 235: Big Data Analytics\n",
    "## Professor Tashman\n",
    "## Project 3: Linear Regression Modeling of California Home Prices\n",
    "Last updated: Jan 31, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instructions\n",
    "In this project, you will work with the California Home Price dataset.  You will train a regression model to predict median home prices.  \n",
    "\n",
    "Learning Objectives\n",
    "Students will gain additional expertise in the following:\n",
    "\n",
    "RDDs, DataFrames, data preprocessing, feature engineering, model training, model evalulation\n",
    "\n",
    "## Lab Exercises (TOTAL POINT VALUE: 10PTS)\n",
    "\n",
    "1) Go through all code and fill in the missing cells. This will prep data, train a model, predict, and evaluate model fit.  Compute and report the Mean Squared Error (MSE) in a table at the very bottom, where all MSE values should be summarized.  \n",
    "Show all work.\n",
    "\n",
    "2) Repeat (1) with at least one additional feature from the original set.\n",
    "3) Repeat (1) with at least one engineered feature based on one or more variables from the original set.  \n",
    "4) Repeat (1) but do Lasso Regression instead of Linear Regression.`\n",
    "\n",
    "\n",
    "### Data Source\n",
    "StatLib---Datasets Archive  \n",
    "http://lib.stat.cmu.edu/datasets/\n",
    "\n",
    "houses.zip\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic covariates. It appeared in Pace and Barry (1997), \"Sparse Spatial Autoregressions\", Statistics and Probability Letters. Submitted by Kelley Pace (kpace@unix1.sncc.lsu.edu). [9/Nov/99] (536 kbytes)\n",
    "\n",
    "\n",
    "\n",
    "Data Description\n",
    "This tutorial makes use of the California Housing data set. It appeared in a 1997 paper titled Sparse Spatial Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. \n",
    "\n",
    "These spatial data contain 20,640 observations on housing prices with 9 economic variables\n",
    "\n",
    "All the block groups with zero entries for the independent and dependent variables have been excluded from the data.\n",
    "\n",
    "The Median house value is the dependent variable and will be assigned the role of the target variable in your ML model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (completed offline by instructor)\n",
    "\n",
    "cadata_raw.txt contains a data description at the top, followed by data.\n",
    "\n",
    "1. Separated data from header  \n",
    "   cal_housing_data_raw.txt  contains only data  \n",
    "   cal_housing_header.txt contains only text\n",
    "2. Some values are in scientific notation.  \n",
    "   Spacing is inconsistent (first 6 fields separated by 2 spaces. Lat/long separated by 1 space)  \n",
    "   Ran the following in Python to format values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = pd.read_csv('/home/ubuntu/projects/cal_housing_data_raw.txt', header=None, sep='  ')\n",
    "\n",
    "d.columns=['v1','v2','v3','v4','v5','v6','v7','v8']  \n",
    "d['latitude'] = d.v8.map(lambda l: float(l.split()[0]))  \n",
    "d['longitude'] = d.v8.map(lambda l: float(l.split()[1]))  \n",
    "d.to_csv('/home/ubuntu/projects/cal_housing_data_preproc.txt', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "read text file, which is comma-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select variables *median_house_value*,*median_income* and show first 5 rows (**1PT**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show descriptive statistics on the following: (**1PT**)  \n",
    "*households*,*median_house_value*,*median_income*,*total_bedrooms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Scale the response variable median_house_value, dividing by 100000 and saving into column *median_house_value_final* (**1PT**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Select the following features into one \"feature\" column, and store in dataframe with scaled response variables:  \n",
    "*median_house_value_final*, *total_bedrooms*, *population*, *households*, *median_income*, *rooms_per_household*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Scale the features (**1PT**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train 80%, test 20% sets, using `seed=314`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a linear regression object with these parameters:  \n",
    "`maxIter=10` `regParam=0.3` `elasticNetParam=0.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each datapoint in the test set, make a prediction (hint: apply `transform()` to the model).\n",
    "You will want the returned object to be a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataframe to an rdd. Then select only the `prediction` and `label` fields (hint: use map()) (**2PTS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model by computing Mean Squared Error (MSE), which is the average sum of squared differences between predicted and label. \n",
    "\n",
    "This can be computed in a single line using `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all MSE values in a table at bottom, indicating run1, run2, run3, run4 (**each MSE worth 1PT for 4PTS total**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
